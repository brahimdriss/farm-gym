{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c030a571",
   "metadata": {
    "id": "c030a571"
   },
   "source": [
    "In this notebook, we use PPO RL agent to learn crop-management policy on a farm constructed with farm-gym.\n",
    "\n",
    "Remark that due to the sotchastic nature of the environment, there are seed for which the agent do not learn correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i6hRAwfDzumm",
   "metadata": {
    "id": "i6hRAwfDzumm"
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QzDrNg5JzwtO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QzDrNg5JzwtO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d88cfc22-d238-4979-a933-1641c1075ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'farm-gym-games'...\n",
      "remote: Enumerating objects: 113, done.\u001b[K\n",
      "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
      "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
      "remote: Total 113 (delta 53), reused 82 (delta 26), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (113/113), 62.48 KiB | 864.00 KiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "/content/farm-gym-games\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Obtaining file:///content/farm-gym-games\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting farmgym@ git+https://github.com/farm-gym/farm-gym\n",
      "  Cloning https://github.com/farm-gym/farm-gym to /tmp/pip-install-2ck9gnwr/farmgym_0abee2233f5a405c8284c3f55136fb45\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/farm-gym/farm-gym /tmp/pip-install-2ck9gnwr/farmgym_0abee2233f5a405c8284c3f55136fb45\n",
      "  Resolved https://github.com/farm-gym/farm-gym to commit ff918cc5ae267ff82b3454373dea6076602529c9\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from farm-gym-games==0.1) (1.13.1+cu116)\n",
      "Requirement already satisfied: gym>=0.25 in /usr/local/lib/python3.8/dist-packages (from farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (0.25.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (1.7.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (3.2.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (7.1.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (1.21.6)\n",
      "Collecting mpld3\n",
      "  Downloading mpld3-0.5.9-py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.2/201.2 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (1.3.5)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (4.6.0.66)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->farm-gym-games==0.1) (4.4.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.25->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.25->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.25->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (6.0.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (0.11.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mpld3->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (2.11.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (2022.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.25->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (3.12.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->mpld3->farmgym@ git+https://github.com/farm-gym/farm-gym->farm-gym-games==0.1) (2.0.1)\n",
      "Building wheels for collected packages: farmgym\n",
      "  Building wheel for farmgym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for farmgym: filename=farmgym-0.2.9-py3-none-any.whl size=211174 sha256=d3180048640bd50b211dbbd79518563d321598e87025b7bb52953cc687eeec0c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7rrelpcx/wheels/43/94/0d/824a38a6ae7b29cb9692d9453f9c0c8c47f4cce8d60a90e7f1\n",
      "Successfully built farmgym\n",
      "Installing collected packages: mpld3, farmgym, farm-gym-games\n",
      "  Running setup.py develop for farm-gym-games\n",
      "Successfully installed farm-gym-games-0.1 farmgym-0.2.9 mpld3-0.5.9\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting rlberry\n",
      "  Downloading rlberry-0.4.0-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.7/294.7 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from rlberry) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from rlberry) (1.21.6)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from rlberry) (0.11.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from rlberry) (3.2.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from rlberry) (6.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from rlberry) (0.3.6)\n",
      "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.8/dist-packages (from rlberry) (1.7.3)\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from rlberry) (1.3.5)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->rlberry) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->rlberry) (6.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->rlberry) (0.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->rlberry) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->rlberry) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->rlberry) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->rlberry) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->rlberry) (2022.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->rlberry) (3.12.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->rlberry) (1.15.0)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=18aa74e852714edf655778e884e297cff2cd9d575f87479c92d8727eba3b4054\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, pygame, rlberry\n",
      "Successfully installed docopt-0.6.2 pygame-2.1.2 rlberry-0.4.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (6.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.12.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827649 sha256=089c5307cfa80301388f805dcdc1f7d5d907dc9fbd059890f3edc58111eff41c\n",
      "  Stored in directory: /root/.cache/pip/wheels/17/79/65/7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
      "Successfully built gym\n",
      "Installing collected packages: gym\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.25.2\n",
      "    Uninstalling gym-0.25.2:\n",
      "      Successfully uninstalled gym-0.25.2\n",
      "Successfully installed gym-0.26.2\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/farm-gym/farm-gym-games\n",
    "%cd farm-gym-games\n",
    "!pip install -e .\n",
    "!pip install rlberry\n",
    "!pip install -U gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13961cc2",
   "metadata": {
    "id": "13961cc2"
   },
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712e8f8",
   "metadata": {
    "id": "4712e8f8"
   },
   "outputs": [],
   "source": [
    "from rlberry.agents.torch import PPOAgent\n",
    "from rlberry.manager import AgentManager, evaluate_agents, plot_writer_data\n",
    "from rlberry.envs import gym_make\n",
    "from rlberry.agents.torch.utils.training import model_factory_from_env\n",
    "\n",
    "\n",
    "import farmgym_games\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10adde",
   "metadata": {
    "id": "5f10adde"
   },
   "source": [
    "### Settings :\n",
    "We'll use the `Farm1` environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d28c7",
   "metadata": {
    "id": "612d28c7"
   },
   "outputs": [],
   "source": [
    "env_ctor, env_kwargs = gym_make, {\"id\": \"OldV21Farm1-v0\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc2942-6c55-4f8b-8f4a-ba3063170e52",
   "metadata": {
    "id": "ccfc2942-6c55-4f8b-8f4a-ba3063170e52"
   },
   "source": [
    "We use an architecture of $256\\times 256$ for both the value and policy neural \n",
    "network of ppo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c367b-1248-43a4-b970-f18da3c16385",
   "metadata": {
    "id": "407c367b-1248-43a4-b970-f18da3c16385"
   },
   "outputs": [],
   "source": [
    "\n",
    "policy_configs = {\n",
    "    \"type\": \"MultiLayerPerceptron\",  # A network architecture\n",
    "    \"layer_sizes\": (256, 256),  # Network dimensions\n",
    "    \"reshape\": False,\n",
    "    \"is_policy\": True,\n",
    "}\n",
    "\n",
    "value_configs = {\n",
    "    \"type\": \"MultiLayerPerceptron\",\n",
    "    \"layer_sizes\": (256, 256),\n",
    "    \"reshape\": False,\n",
    "    \"out_size\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8127f",
   "metadata": {
    "id": "b8b8127f"
   },
   "source": [
    "### Agent code:\n",
    "We use rlberry's [PPOAgent](https://rlberry.readthedocs.io/en/latest/generated/rlberry.agents.torch.PPOAgent.html#rlberry.agents.torch.PPOAgent). \n",
    "Remark that 365 days is the maximum lenght of an episode. This helps us to fix some of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab734a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dab734a",
    "outputId": "0f131668-4763-4b56-baf1-e69cb8252f04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m[INFO] 18:15: Running AgentManager fit() for PPOAgent with n_fit = 1 and max_workers = None. \u001b[0m\n",
      "INFO:rlberry_logger:Running AgentManager fit() for PPOAgent with n_fit = 1 and max_workers = None.\n",
      "/usr/local/lib/python3.8/dist-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "\u001b[38;21m[INFO] 18:16:        agent_name  worker  episode_rewards  max_global_step\n",
      "                      PPOAgent     0          0.0               24 \u001b[0m\n",
      "INFO:rlberry_logger:       agent_name  worker  episode_rewards  max_global_step\n",
      "                      PPOAgent     0          0.0               24\n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 595 | episode_rewards = 0.0 | total_episodes = 5 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 595 | episode_rewards = 0.0 | total_episodes = 5 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 1225 | episode_rewards = 0.0 | total_episodes = 9 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 1225 | episode_rewards = 0.0 | total_episodes = 9 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 1589 | episode_rewards = 0.0 | total_episodes = 12 | fit/surrogate_loss = -2.3878180980682373 | fit/entropy_loss = 0.8597245216369629 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 1589 | episode_rewards = 0.0 | total_episodes = 12 | fit/surrogate_loss = -2.3878180980682373 | fit/entropy_loss = 0.8597245216369629 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 2483 | episode_rewards = 0.0 | total_episodes = 20 | fit/surrogate_loss = 0.41312551498413086 | fit/entropy_loss = 0.8597245216369629 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 2483 | episode_rewards = 0.0 | total_episodes = 20 | fit/surrogate_loss = 0.41312551498413086 | fit/entropy_loss = 0.8597245216369629 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 3622 | episode_rewards = 0.0 | total_episodes = 30 | fit/surrogate_loss = 6.4686174392700195 | fit/entropy_loss = 0.8509177565574646 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 3622 | episode_rewards = 0.0 | total_episodes = 30 | fit/surrogate_loss = 6.4686174392700195 | fit/entropy_loss = 0.8509177565574646 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 4888 | episode_rewards = 0.0 | total_episodes = 38 | fit/surrogate_loss = 6.4686174392700195 | fit/entropy_loss = 0.7864699959754944 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 4888 | episode_rewards = 0.0 | total_episodes = 38 | fit/surrogate_loss = 6.4686174392700195 | fit/entropy_loss = 0.7864699959754944 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 5783 | episode_rewards = 0.0 | total_episodes = 46 | fit/surrogate_loss = 3.164475202560425 | fit/entropy_loss = 0.770107090473175 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 5783 | episode_rewards = 0.0 | total_episodes = 46 | fit/surrogate_loss = 3.164475202560425 | fit/entropy_loss = 0.770107090473175 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 7023 | episode_rewards = 0.0 | total_episodes = 55 | fit/surrogate_loss = -5.5710272789001465 | fit/entropy_loss = 0.8404186367988586 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 7023 | episode_rewards = 0.0 | total_episodes = 55 | fit/surrogate_loss = -5.5710272789001465 | fit/entropy_loss = 0.8404186367988586 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 8086 | episode_rewards = 0.0 | total_episodes = 65 | fit/surrogate_loss = 4.584426403045654 | fit/entropy_loss = 0.7192698121070862 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 8086 | episode_rewards = 0.0 | total_episodes = 65 | fit/surrogate_loss = 4.584426403045654 | fit/entropy_loss = 0.7192698121070862 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 8966 | episode_rewards = 0.0 | total_episodes = 75 | fit/surrogate_loss = -3.6929218769073486 | fit/entropy_loss = 0.8133061528205872 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 8966 | episode_rewards = 0.0 | total_episodes = 75 | fit/surrogate_loss = -3.6929218769073486 | fit/entropy_loss = 0.8133061528205872 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 9920 | episode_rewards = 103.14185619275429 | total_episodes = 84 | fit/surrogate_loss = -3.6929218769073486 | fit/entropy_loss = 0.8133061528205872 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 9920 | episode_rewards = 103.14185619275429 | total_episodes = 84 | fit/surrogate_loss = -3.6929218769073486 | fit/entropy_loss = 0.8133061528205872 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 10997 | episode_rewards = -296.0 | total_episodes = 94 | fit/surrogate_loss = -1.3191218376159668 | fit/entropy_loss = 0.8363655805587769 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 10997 | episode_rewards = -296.0 | total_episodes = 94 | fit/surrogate_loss = -1.3191218376159668 | fit/entropy_loss = 0.8363655805587769 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 11974 | episode_rewards = 22.82941552459888 | total_episodes = 101 | fit/surrogate_loss = -3.9565773010253906 | fit/entropy_loss = 0.6972083449363708 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 11974 | episode_rewards = 22.82941552459888 | total_episodes = 101 | fit/surrogate_loss = -3.9565773010253906 | fit/entropy_loss = 0.6972083449363708 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 13014 | episode_rewards = 0.0 | total_episodes = 110 | fit/surrogate_loss = 2.4154810905456543 | fit/entropy_loss = 0.7834489941596985 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 13014 | episode_rewards = 0.0 | total_episodes = 110 | fit/surrogate_loss = 2.4154810905456543 | fit/entropy_loss = 0.7834489941596985 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 13820 | episode_rewards = 0.0 | total_episodes = 117 | fit/surrogate_loss = -7.054026126861572 | fit/entropy_loss = 0.7395200729370117 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 13820 | episode_rewards = 0.0 | total_episodes = 117 | fit/surrogate_loss = -7.054026126861572 | fit/entropy_loss = 0.7395200729370117 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 14903 | episode_rewards = -0.21665928386681088 | total_episodes = 128 | fit/surrogate_loss = 13.767631530761719 | fit/entropy_loss = 0.7395200729370117 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 14903 | episode_rewards = -0.21665928386681088 | total_episodes = 128 | fit/surrogate_loss = 13.767631530761719 | fit/entropy_loss = 0.7395200729370117 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 16176 | episode_rewards = 0.0 | total_episodes = 137 | fit/surrogate_loss = 13.767631530761719 | fit/entropy_loss = 0.6276310086250305 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 16176 | episode_rewards = 0.0 | total_episodes = 137 | fit/surrogate_loss = 13.767631530761719 | fit/entropy_loss = 0.6276310086250305 | \n",
      "\u001b[38;21m[INFO] 18:16: [PPOAgent[worker: 0]] | max_global_step = 17194 | episode_rewards = 0.0 | total_episodes = 146 | fit/surrogate_loss = 1.7612954378128052 | fit/entropy_loss = 0.6631001234054565 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 17194 | episode_rewards = 0.0 | total_episodes = 146 | fit/surrogate_loss = 1.7612954378128052 | fit/entropy_loss = 0.6631001234054565 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 17982 | episode_rewards = 0.0 | total_episodes = 153 | fit/surrogate_loss = -3.994690418243408 | fit/entropy_loss = 0.7903012037277222 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 17982 | episode_rewards = 0.0 | total_episodes = 153 | fit/surrogate_loss = -3.994690418243408 | fit/entropy_loss = 0.7903012037277222 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 19095 | episode_rewards = 0.0 | total_episodes = 161 | fit/surrogate_loss = 4.2243332862854 | fit/entropy_loss = 0.676083505153656 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 19095 | episode_rewards = 0.0 | total_episodes = 161 | fit/surrogate_loss = 4.2243332862854 | fit/entropy_loss = 0.676083505153656 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 20278 | episode_rewards = 0.0 | total_episodes = 172 | fit/surrogate_loss = -3.0555825233459473 | fit/entropy_loss = 0.7409288287162781 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 20278 | episode_rewards = 0.0 | total_episodes = 172 | fit/surrogate_loss = -3.0555825233459473 | fit/entropy_loss = 0.7409288287162781 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 21494 | episode_rewards = 358.220287091834 | total_episodes = 181 | fit/surrogate_loss = -4.634044170379639 | fit/entropy_loss = 0.7709406018257141 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 21494 | episode_rewards = 358.220287091834 | total_episodes = 181 | fit/surrogate_loss = -4.634044170379639 | fit/entropy_loss = 0.7709406018257141 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 22460 | episode_rewards = 0.0 | total_episodes = 189 | fit/surrogate_loss = -4.634044170379639 | fit/entropy_loss = 0.7709406018257141 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 22460 | episode_rewards = 0.0 | total_episodes = 189 | fit/surrogate_loss = -4.634044170379639 | fit/entropy_loss = 0.7709406018257141 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 23226 | episode_rewards = 0.0 | total_episodes = 196 | fit/surrogate_loss = 1.5475878715515137 | fit/entropy_loss = 0.7258386015892029 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 23226 | episode_rewards = 0.0 | total_episodes = 196 | fit/surrogate_loss = 1.5475878715515137 | fit/entropy_loss = 0.7258386015892029 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 24151 | episode_rewards = 0.0 | total_episodes = 203 | fit/surrogate_loss = 8.277814865112305 | fit/entropy_loss = 0.6978229880332947 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 24151 | episode_rewards = 0.0 | total_episodes = 203 | fit/surrogate_loss = 8.277814865112305 | fit/entropy_loss = 0.6978229880332947 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 25181 | episode_rewards = 98.99288527283124 | total_episodes = 211 | fit/surrogate_loss = 6.71817684173584 | fit/entropy_loss = 0.7196332216262817 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 25181 | episode_rewards = 98.99288527283124 | total_episodes = 211 | fit/surrogate_loss = 6.71817684173584 | fit/entropy_loss = 0.7196332216262817 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 26060 | episode_rewards = 0.0 | total_episodes = 218 | fit/surrogate_loss = 6.71817684173584 | fit/entropy_loss = 0.7196332216262817 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 26060 | episode_rewards = 0.0 | total_episodes = 218 | fit/surrogate_loss = 6.71817684173584 | fit/entropy_loss = 0.7196332216262817 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 27142 | episode_rewards = -96.0 | total_episodes = 228 | fit/surrogate_loss = -2.6381707191467285 | fit/entropy_loss = 0.719673216342926 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 27142 | episode_rewards = -96.0 | total_episodes = 228 | fit/surrogate_loss = -2.6381707191467285 | fit/entropy_loss = 0.719673216342926 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 28317 | episode_rewards = 0.0 | total_episodes = 237 | fit/surrogate_loss = -0.36613813042640686 | fit/entropy_loss = 0.7670795321464539 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 28317 | episode_rewards = 0.0 | total_episodes = 237 | fit/surrogate_loss = -0.36613813042640686 | fit/entropy_loss = 0.7670795321464539 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 29475 | episode_rewards = 0.0 | total_episodes = 246 | fit/surrogate_loss = -0.564029335975647 | fit/entropy_loss = 0.7156108617782593 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 29475 | episode_rewards = 0.0 | total_episodes = 246 | fit/surrogate_loss = -0.564029335975647 | fit/entropy_loss = 0.7156108617782593 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 30306 | episode_rewards = 0.0 | total_episodes = 252 | fit/surrogate_loss = 1.68136465549469 | fit/entropy_loss = 0.625684380531311 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 30306 | episode_rewards = 0.0 | total_episodes = 252 | fit/surrogate_loss = 1.68136465549469 | fit/entropy_loss = 0.625684380531311 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 31211 | episode_rewards = 0.0 | total_episodes = 260 | fit/surrogate_loss = -1.7476184368133545 | fit/entropy_loss = 0.625684380531311 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 31211 | episode_rewards = 0.0 | total_episodes = 260 | fit/surrogate_loss = -1.7476184368133545 | fit/entropy_loss = 0.625684380531311 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 32259 | episode_rewards = 0.0 | total_episodes = 267 | fit/surrogate_loss = -1.7476184368133545 | fit/entropy_loss = 0.6527275443077087 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 32259 | episode_rewards = 0.0 | total_episodes = 267 | fit/surrogate_loss = -1.7476184368133545 | fit/entropy_loss = 0.6527275443077087 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 33353 | episode_rewards = 0.0 | total_episodes = 276 | fit/surrogate_loss = -3.3000216484069824 | fit/entropy_loss = 0.6524675488471985 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 33353 | episode_rewards = 0.0 | total_episodes = 276 | fit/surrogate_loss = -3.3000216484069824 | fit/entropy_loss = 0.6524675488471985 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 34116 | episode_rewards = 0.0 | total_episodes = 281 | fit/surrogate_loss = -1.9277983903884888 | fit/entropy_loss = 0.6326231956481934 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 34116 | episode_rewards = 0.0 | total_episodes = 281 | fit/surrogate_loss = -1.9277983903884888 | fit/entropy_loss = 0.6326231956481934 | \n",
      "\u001b[38;21m[INFO] 18:17: [PPOAgent[worker: 0]] | max_global_step = 35205 | episode_rewards = 132.27549496534468 | total_episodes = 290 | fit/surrogate_loss = -1.043172836303711 | fit/entropy_loss = 0.7019397020339966 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 35205 | episode_rewards = 132.27549496534468 | total_episodes = 290 | fit/surrogate_loss = -1.043172836303711 | fit/entropy_loss = 0.7019397020339966 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 36410 | episode_rewards = 0.0 | total_episodes = 299 | fit/surrogate_loss = -2.607722759246826 | fit/entropy_loss = 0.660306453704834 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 36410 | episode_rewards = 0.0 | total_episodes = 299 | fit/surrogate_loss = -2.607722759246826 | fit/entropy_loss = 0.660306453704834 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 37451 | episode_rewards = 0.0 | total_episodes = 308 | fit/surrogate_loss = -2.0025434494018555 | fit/entropy_loss = 0.660306453704834 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 37451 | episode_rewards = 0.0 | total_episodes = 308 | fit/surrogate_loss = -2.0025434494018555 | fit/entropy_loss = 0.660306453704834 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 38425 | episode_rewards = 0.0 | total_episodes = 316 | fit/surrogate_loss = -2.0025434494018555 | fit/entropy_loss = 0.6660458445549011 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 38425 | episode_rewards = 0.0 | total_episodes = 316 | fit/surrogate_loss = -2.0025434494018555 | fit/entropy_loss = 0.6660458445549011 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 39567 | episode_rewards = 0.0 | total_episodes = 324 | fit/surrogate_loss = -2.566422939300537 | fit/entropy_loss = 0.6672859191894531 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 39567 | episode_rewards = 0.0 | total_episodes = 324 | fit/surrogate_loss = -2.566422939300537 | fit/entropy_loss = 0.6672859191894531 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 40705 | episode_rewards = 0.0 | total_episodes = 333 | fit/surrogate_loss = 5.434147834777832 | fit/entropy_loss = 0.5890029072761536 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 40705 | episode_rewards = 0.0 | total_episodes = 333 | fit/surrogate_loss = 5.434147834777832 | fit/entropy_loss = 0.5890029072761536 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 41750 | episode_rewards = 0.0 | total_episodes = 342 | fit/surrogate_loss = -1.3875102996826172 | fit/entropy_loss = 0.7163137197494507 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 41750 | episode_rewards = 0.0 | total_episodes = 342 | fit/surrogate_loss = -1.3875102996826172 | fit/entropy_loss = 0.7163137197494507 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 42587 | episode_rewards = 0.0 | total_episodes = 350 | fit/surrogate_loss = -2.7725677490234375 | fit/entropy_loss = 0.6286504864692688 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 42587 | episode_rewards = 0.0 | total_episodes = 350 | fit/surrogate_loss = -2.7725677490234375 | fit/entropy_loss = 0.6286504864692688 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 43625 | episode_rewards = 0.0 | total_episodes = 359 | fit/surrogate_loss = 4.865692615509033 | fit/entropy_loss = 0.6286504864692688 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 43625 | episode_rewards = 0.0 | total_episodes = 359 | fit/surrogate_loss = 4.865692615509033 | fit/entropy_loss = 0.6286504864692688 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 44980 | episode_rewards = 0.0 | total_episodes = 370 | fit/surrogate_loss = 4.865692615509033 | fit/entropy_loss = 0.6463263630867004 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 44980 | episode_rewards = 0.0 | total_episodes = 370 | fit/surrogate_loss = 4.865692615509033 | fit/entropy_loss = 0.6463263630867004 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 46124 | episode_rewards = -164.0 | total_episodes = 381 | fit/surrogate_loss = -2.4584341049194336 | fit/entropy_loss = 0.7417486906051636 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 46124 | episode_rewards = -164.0 | total_episodes = 381 | fit/surrogate_loss = -2.4584341049194336 | fit/entropy_loss = 0.7417486906051636 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 46933 | episode_rewards = 0.0 | total_episodes = 387 | fit/surrogate_loss = -5.630449295043945 | fit/entropy_loss = 0.6848984956741333 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 46933 | episode_rewards = 0.0 | total_episodes = 387 | fit/surrogate_loss = -5.630449295043945 | fit/entropy_loss = 0.6848984956741333 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 48028 | episode_rewards = 0.0 | total_episodes = 396 | fit/surrogate_loss = -0.9338213801383972 | fit/entropy_loss = 0.6848537921905518 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 48028 | episode_rewards = 0.0 | total_episodes = 396 | fit/surrogate_loss = -0.9338213801383972 | fit/entropy_loss = 0.6848537921905518 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 49081 | episode_rewards = 0.0 | total_episodes = 406 | fit/surrogate_loss = -1.8852843046188354 | fit/entropy_loss = 0.7805490493774414 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 49081 | episode_rewards = 0.0 | total_episodes = 406 | fit/surrogate_loss = -1.8852843046188354 | fit/entropy_loss = 0.7805490493774414 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 50206 | episode_rewards = 61.83304145665479 | total_episodes = 416 | fit/surrogate_loss = -1.6394283771514893 | fit/entropy_loss = 0.7155368328094482 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 50206 | episode_rewards = 61.83304145665479 | total_episodes = 416 | fit/surrogate_loss = -1.6394283771514893 | fit/entropy_loss = 0.7155368328094482 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 51065 | episode_rewards = 0.0 | total_episodes = 423 | fit/surrogate_loss = -1.6394283771514893 | fit/entropy_loss = 0.7155368328094482 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 51065 | episode_rewards = 0.0 | total_episodes = 423 | fit/surrogate_loss = -1.6394283771514893 | fit/entropy_loss = 0.7155368328094482 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 52192 | episode_rewards = 0.0 | total_episodes = 433 | fit/surrogate_loss = 4.461411476135254 | fit/entropy_loss = 0.7399415373802185 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 52192 | episode_rewards = 0.0 | total_episodes = 433 | fit/surrogate_loss = 4.461411476135254 | fit/entropy_loss = 0.7399415373802185 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 53264 | episode_rewards = 0.0 | total_episodes = 442 | fit/surrogate_loss = 4.093235969543457 | fit/entropy_loss = 0.6826639771461487 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 53264 | episode_rewards = 0.0 | total_episodes = 442 | fit/surrogate_loss = 4.093235969543457 | fit/entropy_loss = 0.6826639771461487 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 54333 | episode_rewards = 27.728366070123705 | total_episodes = 450 | fit/surrogate_loss = 6.2170729637146 | fit/entropy_loss = 0.7634309530258179 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 54333 | episode_rewards = 27.728366070123705 | total_episodes = 450 | fit/surrogate_loss = 6.2170729637146 | fit/entropy_loss = 0.7634309530258179 | \n",
      "\u001b[38;21m[INFO] 18:18: [PPOAgent[worker: 0]] | max_global_step = 55131 | episode_rewards = 0.0 | total_episodes = 458 | fit/surrogate_loss = 3.745279550552368 | fit/entropy_loss = 0.7220866084098816 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 55131 | episode_rewards = 0.0 | total_episodes = 458 | fit/surrogate_loss = 3.745279550552368 | fit/entropy_loss = 0.7220866084098816 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 56148 | episode_rewards = 0.0 | total_episodes = 468 | fit/surrogate_loss = -2.873089075088501 | fit/entropy_loss = 0.7220866084098816 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 56148 | episode_rewards = 0.0 | total_episodes = 468 | fit/surrogate_loss = -2.873089075088501 | fit/entropy_loss = 0.7220866084098816 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 57461 | episode_rewards = 0.0 | total_episodes = 477 | fit/surrogate_loss = -2.873089075088501 | fit/entropy_loss = 0.7668716907501221 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 57461 | episode_rewards = 0.0 | total_episodes = 477 | fit/surrogate_loss = -2.873089075088501 | fit/entropy_loss = 0.7668716907501221 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 58571 | episode_rewards = 0.0 | total_episodes = 485 | fit/surrogate_loss = -1.9949129819869995 | fit/entropy_loss = 0.7874887585639954 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 58571 | episode_rewards = 0.0 | total_episodes = 485 | fit/surrogate_loss = -1.9949129819869995 | fit/entropy_loss = 0.7874887585639954 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 59313 | episode_rewards = 66.85562377389793 | total_episodes = 491 | fit/surrogate_loss = 0.1627015471458435 | fit/entropy_loss = 0.7222691774368286 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 59313 | episode_rewards = 66.85562377389793 | total_episodes = 491 | fit/surrogate_loss = 0.1627015471458435 | fit/entropy_loss = 0.7222691774368286 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 60420 | episode_rewards = 0.0 | total_episodes = 500 | fit/surrogate_loss = 5.812366485595703 | fit/entropy_loss = 0.6882644295692444 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 60420 | episode_rewards = 0.0 | total_episodes = 500 | fit/surrogate_loss = 5.812366485595703 | fit/entropy_loss = 0.6882644295692444 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 61512 | episode_rewards = 0.0 | total_episodes = 508 | fit/surrogate_loss = -4.415796756744385 | fit/entropy_loss = 0.7235307693481445 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 61512 | episode_rewards = 0.0 | total_episodes = 508 | fit/surrogate_loss = -4.415796756744385 | fit/entropy_loss = 0.7235307693481445 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 62660 | episode_rewards = 0.0 | total_episodes = 517 | fit/surrogate_loss = -2.4717729091644287 | fit/entropy_loss = 0.6634413599967957 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 62660 | episode_rewards = 0.0 | total_episodes = 517 | fit/surrogate_loss = -2.4717729091644287 | fit/entropy_loss = 0.6634413599967957 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 63477 | episode_rewards = 0.0 | total_episodes = 523 | fit/surrogate_loss = -2.4717729091644287 | fit/entropy_loss = 0.6634413599967957 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 63477 | episode_rewards = 0.0 | total_episodes = 523 | fit/surrogate_loss = -2.4717729091644287 | fit/entropy_loss = 0.6634413599967957 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 64590 | episode_rewards = 0.0 | total_episodes = 532 | fit/surrogate_loss = 0.7364750504493713 | fit/entropy_loss = 0.7166408896446228 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 64590 | episode_rewards = 0.0 | total_episodes = 532 | fit/surrogate_loss = 0.7364750504493713 | fit/entropy_loss = 0.7166408896446228 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 65660 | episode_rewards = 0.0 | total_episodes = 540 | fit/surrogate_loss = -2.438608407974243 | fit/entropy_loss = 0.6841661334037781 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 65660 | episode_rewards = 0.0 | total_episodes = 540 | fit/surrogate_loss = -2.438608407974243 | fit/entropy_loss = 0.6841661334037781 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 66776 | episode_rewards = 0.0 | total_episodes = 548 | fit/surrogate_loss = -1.7258297204971313 | fit/entropy_loss = 0.7338957786560059 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 66776 | episode_rewards = 0.0 | total_episodes = 548 | fit/surrogate_loss = -1.7258297204971313 | fit/entropy_loss = 0.7338957786560059 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 67473 | episode_rewards = 228.47886846927662 | total_episodes = 554 | fit/surrogate_loss = 7.729416370391846 | fit/entropy_loss = 0.7338957786560059 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 67473 | episode_rewards = 228.47886846927662 | total_episodes = 554 | fit/surrogate_loss = 7.729416370391846 | fit/entropy_loss = 0.7338957786560059 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 68730 | episode_rewards = 77.23046014367402 | total_episodes = 563 | fit/surrogate_loss = 7.729416370391846 | fit/entropy_loss = 0.7086657881736755 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 68730 | episode_rewards = 77.23046014367402 | total_episodes = 563 | fit/surrogate_loss = 7.729416370391846 | fit/entropy_loss = 0.7086657881736755 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 69823 | episode_rewards = 0.0 | total_episodes = 571 | fit/surrogate_loss = -1.287868618965149 | fit/entropy_loss = 0.7333556413650513 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 69823 | episode_rewards = 0.0 | total_episodes = 571 | fit/surrogate_loss = -1.287868618965149 | fit/entropy_loss = 0.7333556413650513 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 70953 | episode_rewards = 55.74819293272816 | total_episodes = 581 | fit/surrogate_loss = -0.622504711151123 | fit/entropy_loss = 0.6846259832382202 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 70953 | episode_rewards = 55.74819293272816 | total_episodes = 581 | fit/surrogate_loss = -0.622504711151123 | fit/entropy_loss = 0.6846259832382202 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 71739 | episode_rewards = 0.0 | total_episodes = 588 | fit/surrogate_loss = -1.0952839851379395 | fit/entropy_loss = 0.7975064516067505 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 71739 | episode_rewards = 0.0 | total_episodes = 588 | fit/surrogate_loss = -1.0952839851379395 | fit/entropy_loss = 0.7975064516067505 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 72882 | episode_rewards = 0.0 | total_episodes = 597 | fit/surrogate_loss = 5.241018772125244 | fit/entropy_loss = 0.7693597078323364 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 72882 | episode_rewards = 0.0 | total_episodes = 597 | fit/surrogate_loss = 5.241018772125244 | fit/entropy_loss = 0.7693597078323364 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 73951 | episode_rewards = 16.0 | total_episodes = 605 | fit/surrogate_loss = -1.9436891078948975 | fit/entropy_loss = 0.7547474503517151 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 73951 | episode_rewards = 16.0 | total_episodes = 605 | fit/surrogate_loss = -1.9436891078948975 | fit/entropy_loss = 0.7547474503517151 | \n",
      "\u001b[38;21m[INFO] 18:19: [PPOAgent[worker: 0]] | max_global_step = 75063 | episode_rewards = 0.0 | total_episodes = 613 | fit/surrogate_loss = -1.8972619771957397 | fit/entropy_loss = 0.6869950890541077 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 75063 | episode_rewards = 0.0 | total_episodes = 613 | fit/surrogate_loss = -1.8972619771957397 | fit/entropy_loss = 0.6869950890541077 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 75961 | episode_rewards = 0.0 | total_episodes = 621 | fit/surrogate_loss = -1.8972619771957397 | fit/entropy_loss = 0.6869950890541077 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 75961 | episode_rewards = 0.0 | total_episodes = 621 | fit/surrogate_loss = -1.8972619771957397 | fit/entropy_loss = 0.6869950890541077 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 77096 | episode_rewards = 0.0 | total_episodes = 631 | fit/surrogate_loss = 5.951220512390137 | fit/entropy_loss = 0.7643617391586304 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 77096 | episode_rewards = 0.0 | total_episodes = 631 | fit/surrogate_loss = 5.951220512390137 | fit/entropy_loss = 0.7643617391586304 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 78266 | episode_rewards = 323.3316880912997 | total_episodes = 641 | fit/surrogate_loss = 0.30267244577407837 | fit/entropy_loss = 0.7650519013404846 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 78266 | episode_rewards = 323.3316880912997 | total_episodes = 641 | fit/surrogate_loss = 0.30267244577407837 | fit/entropy_loss = 0.7650519013404846 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 79416 | episode_rewards = 158.40753859271803 | total_episodes = 652 | fit/surrogate_loss = 0.5060651302337646 | fit/entropy_loss = 0.8244049549102783 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 79416 | episode_rewards = 158.40753859271803 | total_episodes = 652 | fit/surrogate_loss = 0.5060651302337646 | fit/entropy_loss = 0.8244049549102783 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 80190 | episode_rewards = 0.0 | total_episodes = 658 | fit/surrogate_loss = -2.0423691272735596 | fit/entropy_loss = 0.7359488606452942 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 80190 | episode_rewards = 0.0 | total_episodes = 658 | fit/surrogate_loss = -2.0423691272735596 | fit/entropy_loss = 0.7359488606452942 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 81197 | episode_rewards = 0.0 | total_episodes = 667 | fit/surrogate_loss = -4.146605014801025 | fit/entropy_loss = 0.7359488606452942 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 81197 | episode_rewards = 0.0 | total_episodes = 667 | fit/surrogate_loss = -4.146605014801025 | fit/entropy_loss = 0.7359488606452942 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 82407 | episode_rewards = 89.40103844667948 | total_episodes = 677 | fit/surrogate_loss = -4.146605014801025 | fit/entropy_loss = 0.751126766204834 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 82407 | episode_rewards = 89.40103844667948 | total_episodes = 677 | fit/surrogate_loss = -4.146605014801025 | fit/entropy_loss = 0.751126766204834 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 83552 | episode_rewards = 0.0 | total_episodes = 686 | fit/surrogate_loss = -1.5227149724960327 | fit/entropy_loss = 0.7652062773704529 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 83552 | episode_rewards = 0.0 | total_episodes = 686 | fit/surrogate_loss = -1.5227149724960327 | fit/entropy_loss = 0.7652062773704529 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 84358 | episode_rewards = 0.0 | total_episodes = 692 | fit/surrogate_loss = -1.9792309999465942 | fit/entropy_loss = 0.735310971736908 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 84358 | episode_rewards = 0.0 | total_episodes = 692 | fit/surrogate_loss = -1.9792309999465942 | fit/entropy_loss = 0.735310971736908 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 85387 | episode_rewards = 0.0 | total_episodes = 699 | fit/surrogate_loss = 0.4356931746006012 | fit/entropy_loss = 0.6522723436355591 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 85387 | episode_rewards = 0.0 | total_episodes = 699 | fit/surrogate_loss = 0.4356931746006012 | fit/entropy_loss = 0.6522723436355591 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 86516 | episode_rewards = 0.0 | total_episodes = 708 | fit/surrogate_loss = 4.370786190032959 | fit/entropy_loss = 0.6713957190513611 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 86516 | episode_rewards = 0.0 | total_episodes = 708 | fit/surrogate_loss = 4.370786190032959 | fit/entropy_loss = 0.6713957190513611 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 87647 | episode_rewards = 0.0 | total_episodes = 716 | fit/surrogate_loss = -3.1698312759399414 | fit/entropy_loss = 0.6282444000244141 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 87647 | episode_rewards = 0.0 | total_episodes = 716 | fit/surrogate_loss = -3.1698312759399414 | fit/entropy_loss = 0.6282444000244141 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 88459 | episode_rewards = 163.051141678229 | total_episodes = 722 | fit/surrogate_loss = -3.1698312759399414 | fit/entropy_loss = 0.6282444000244141 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 88459 | episode_rewards = 163.051141678229 | total_episodes = 722 | fit/surrogate_loss = -3.1698312759399414 | fit/entropy_loss = 0.6282444000244141 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 89539 | episode_rewards = 0.0 | total_episodes = 730 | fit/surrogate_loss = 0.12393805384635925 | fit/entropy_loss = 0.6050181984901428 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 89539 | episode_rewards = 0.0 | total_episodes = 730 | fit/surrogate_loss = 0.12393805384635925 | fit/entropy_loss = 0.6050181984901428 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 90705 | episode_rewards = 0.0 | total_episodes = 741 | fit/surrogate_loss = -2.1812515258789062 | fit/entropy_loss = 0.67020183801651 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 90705 | episode_rewards = 0.0 | total_episodes = 741 | fit/surrogate_loss = -2.1812515258789062 | fit/entropy_loss = 0.67020183801651 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 91875 | episode_rewards = 15.0 | total_episodes = 750 | fit/surrogate_loss = -0.8971680402755737 | fit/entropy_loss = 0.7324327826499939 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 91875 | episode_rewards = 15.0 | total_episodes = 750 | fit/surrogate_loss = -0.8971680402755737 | fit/entropy_loss = 0.7324327826499939 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 92665 | episode_rewards = 0.0 | total_episodes = 756 | fit/surrogate_loss = 4.649515628814697 | fit/entropy_loss = 0.6592446565628052 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 92665 | episode_rewards = 0.0 | total_episodes = 756 | fit/surrogate_loss = 4.649515628814697 | fit/entropy_loss = 0.6592446565628052 | \n",
      "\u001b[38;21m[INFO] 18:20: [PPOAgent[worker: 0]] | max_global_step = 93645 | episode_rewards = 0.0 | total_episodes = 765 | fit/surrogate_loss = -1.1357768774032593 | fit/entropy_loss = 0.6592446565628052 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 93645 | episode_rewards = 0.0 | total_episodes = 765 | fit/surrogate_loss = -1.1357768774032593 | fit/entropy_loss = 0.6592446565628052 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 94852 | episode_rewards = 0.0 | total_episodes = 774 | fit/surrogate_loss = 0.5869371294975281 | fit/entropy_loss = 0.7204352021217346 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 94852 | episode_rewards = 0.0 | total_episodes = 774 | fit/surrogate_loss = 0.5869371294975281 | fit/entropy_loss = 0.7204352021217346 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 96223 | episode_rewards = 0.0 | total_episodes = 783 | fit/surrogate_loss = 0.5869371294975281 | fit/entropy_loss = 0.5762943625450134 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 96223 | episode_rewards = 0.0 | total_episodes = 783 | fit/surrogate_loss = 0.5869371294975281 | fit/entropy_loss = 0.5762943625450134 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 97010 | episode_rewards = 0.0 | total_episodes = 789 | fit/surrogate_loss = 0.4796890914440155 | fit/entropy_loss = 0.7007847428321838 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 97010 | episode_rewards = 0.0 | total_episodes = 789 | fit/surrogate_loss = 0.4796890914440155 | fit/entropy_loss = 0.7007847428321838 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 98064 | episode_rewards = 0.0 | total_episodes = 797 | fit/surrogate_loss = 2.36637282371521 | fit/entropy_loss = 0.6839280128479004 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 98064 | episode_rewards = 0.0 | total_episodes = 797 | fit/surrogate_loss = 2.36637282371521 | fit/entropy_loss = 0.6839280128479004 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 99197 | episode_rewards = 0.0 | total_episodes = 805 | fit/surrogate_loss = -2.4720537662506104 | fit/entropy_loss = 0.6613487005233765 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 99197 | episode_rewards = 0.0 | total_episodes = 805 | fit/surrogate_loss = -2.4720537662506104 | fit/entropy_loss = 0.6613487005233765 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 100330 | episode_rewards = 166.9560041446476 | total_episodes = 814 | fit/surrogate_loss = 6.365172863006592 | fit/entropy_loss = 0.7301284670829773 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 100330 | episode_rewards = 166.9560041446476 | total_episodes = 814 | fit/surrogate_loss = 6.365172863006592 | fit/entropy_loss = 0.7301284670829773 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 101144 | episode_rewards = 0.0 | total_episodes = 821 | fit/surrogate_loss = 6.365172863006592 | fit/entropy_loss = 0.7301284670829773 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 101144 | episode_rewards = 0.0 | total_episodes = 821 | fit/surrogate_loss = 6.365172863006592 | fit/entropy_loss = 0.7301284670829773 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 102292 | episode_rewards = 0.0 | total_episodes = 829 | fit/surrogate_loss = 0.27424129843711853 | fit/entropy_loss = 0.7275536060333252 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 102292 | episode_rewards = 0.0 | total_episodes = 829 | fit/surrogate_loss = 0.27424129843711853 | fit/entropy_loss = 0.7275536060333252 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 103374 | episode_rewards = 16.0 | total_episodes = 838 | fit/surrogate_loss = -3.1582345962524414 | fit/entropy_loss = 0.6926942467689514 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 103374 | episode_rewards = 16.0 | total_episodes = 838 | fit/surrogate_loss = -3.1582345962524414 | fit/entropy_loss = 0.6926942467689514 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 104511 | episode_rewards = 0.0 | total_episodes = 847 | fit/surrogate_loss = -2.58957576751709 | fit/entropy_loss = 0.7193616628646851 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 104511 | episode_rewards = 0.0 | total_episodes = 847 | fit/surrogate_loss = -2.58957576751709 | fit/entropy_loss = 0.7193616628646851 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 105275 | episode_rewards = 0.0 | total_episodes = 853 | fit/surrogate_loss = 4.624680995941162 | fit/entropy_loss = 0.6811928749084473 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 105275 | episode_rewards = 0.0 | total_episodes = 853 | fit/surrogate_loss = 4.624680995941162 | fit/entropy_loss = 0.6811928749084473 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 106407 | episode_rewards = 280.3789407878635 | total_episodes = 862 | fit/surrogate_loss = 5.952941417694092 | fit/entropy_loss = 0.6525088548660278 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 106407 | episode_rewards = 280.3789407878635 | total_episodes = 862 | fit/surrogate_loss = 5.952941417694092 | fit/entropy_loss = 0.6525088548660278 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 107381 | episode_rewards = 42.87270374433392 | total_episodes = 873 | fit/surrogate_loss = 2.326967477798462 | fit/entropy_loss = 0.6525088548660278 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 107381 | episode_rewards = 42.87270374433392 | total_episodes = 873 | fit/surrogate_loss = 2.326967477798462 | fit/entropy_loss = 0.6525088548660278 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 108628 | episode_rewards = 0.0 | total_episodes = 882 | fit/surrogate_loss = 7.984739780426025 | fit/entropy_loss = 0.6362832188606262 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 108628 | episode_rewards = 0.0 | total_episodes = 882 | fit/surrogate_loss = 7.984739780426025 | fit/entropy_loss = 0.6362832188606262 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 109551 | episode_rewards = 0.0 | total_episodes = 888 | fit/surrogate_loss = 7.984739780426025 | fit/entropy_loss = 0.6350991725921631 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 109551 | episode_rewards = 0.0 | total_episodes = 888 | fit/surrogate_loss = 7.984739780426025 | fit/entropy_loss = 0.6350991725921631 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 110588 | episode_rewards = 0.0 | total_episodes = 897 | fit/surrogate_loss = 0.7880123853683472 | fit/entropy_loss = 0.6566298604011536 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 110588 | episode_rewards = 0.0 | total_episodes = 897 | fit/surrogate_loss = 0.7880123853683472 | fit/entropy_loss = 0.6566298604011536 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 111664 | episode_rewards = 0.0 | total_episodes = 905 | fit/surrogate_loss = 2.2572789192199707 | fit/entropy_loss = 0.7234114408493042 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 111664 | episode_rewards = 0.0 | total_episodes = 905 | fit/surrogate_loss = 2.2572789192199707 | fit/entropy_loss = 0.7234114408493042 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 112821 | episode_rewards = 101.45158009493954 | total_episodes = 913 | fit/surrogate_loss = -2.5916388034820557 | fit/entropy_loss = 0.6850094199180603 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 112821 | episode_rewards = 101.45158009493954 | total_episodes = 913 | fit/surrogate_loss = -2.5916388034820557 | fit/entropy_loss = 0.6850094199180603 | \n",
      "\u001b[38;21m[INFO] 18:21: [PPOAgent[worker: 0]] | max_global_step = 113663 | episode_rewards = 0.0 | total_episodes = 923 | fit/surrogate_loss = -2.5916388034820557 | fit/entropy_loss = 0.6850094199180603 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 113663 | episode_rewards = 0.0 | total_episodes = 923 | fit/surrogate_loss = -2.5916388034820557 | fit/entropy_loss = 0.6850094199180603 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 114708 | episode_rewards = 0.0 | total_episodes = 931 | fit/surrogate_loss = -2.8471524715423584 | fit/entropy_loss = 0.7169556021690369 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 114708 | episode_rewards = 0.0 | total_episodes = 931 | fit/surrogate_loss = -2.8471524715423584 | fit/entropy_loss = 0.7169556021690369 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 115822 | episode_rewards = 0.0 | total_episodes = 940 | fit/surrogate_loss = -5.277248859405518 | fit/entropy_loss = 0.782137393951416 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 115822 | episode_rewards = 0.0 | total_episodes = 940 | fit/surrogate_loss = -5.277248859405518 | fit/entropy_loss = 0.782137393951416 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 116913 | episode_rewards = 0.0 | total_episodes = 950 | fit/surrogate_loss = -0.4722362756729126 | fit/entropy_loss = 0.7425118088722229 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 116913 | episode_rewards = 0.0 | total_episodes = 950 | fit/surrogate_loss = -0.4722362756729126 | fit/entropy_loss = 0.7425118088722229 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 117720 | episode_rewards = 0.0 | total_episodes = 956 | fit/surrogate_loss = -1.9885547161102295 | fit/entropy_loss = 0.659453272819519 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 117720 | episode_rewards = 0.0 | total_episodes = 956 | fit/surrogate_loss = -1.9885547161102295 | fit/entropy_loss = 0.659453272819519 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 118671 | episode_rewards = 0.0 | total_episodes = 965 | fit/surrogate_loss = 5.828312873840332 | fit/entropy_loss = 0.659453272819519 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 118671 | episode_rewards = 0.0 | total_episodes = 965 | fit/surrogate_loss = 5.828312873840332 | fit/entropy_loss = 0.659453272819519 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 119851 | episode_rewards = 0.0 | total_episodes = 975 | fit/surrogate_loss = 6.466946601867676 | fit/entropy_loss = 0.7149524092674255 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 119851 | episode_rewards = 0.0 | total_episodes = 975 | fit/surrogate_loss = 6.466946601867676 | fit/entropy_loss = 0.7149524092674255 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 121222 | episode_rewards = 0.0 | total_episodes = 985 | fit/surrogate_loss = 6.466946601867676 | fit/entropy_loss = 0.7250706553459167 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 121222 | episode_rewards = 0.0 | total_episodes = 985 | fit/surrogate_loss = 6.466946601867676 | fit/entropy_loss = 0.7250706553459167 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 121959 | episode_rewards = 27.040416067625273 | total_episodes = 992 | fit/surrogate_loss = -1.6796014308929443 | fit/entropy_loss = 0.6767464280128479 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 121959 | episode_rewards = 27.040416067625273 | total_episodes = 992 | fit/surrogate_loss = -1.6796014308929443 | fit/entropy_loss = 0.6767464280128479 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 122703 | episode_rewards = 0.0 | total_episodes = 999 | fit/surrogate_loss = 0.1955440193414688 | fit/entropy_loss = 0.6441108584403992 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 122703 | episode_rewards = 0.0 | total_episodes = 999 | fit/surrogate_loss = 0.1955440193414688 | fit/entropy_loss = 0.6441108584403992 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 123694 | episode_rewards = 157.61078628062057 | total_episodes = 1007 | fit/surrogate_loss = 0.1955440193414688 | fit/entropy_loss = 0.6441108584403992 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 123694 | episode_rewards = 157.61078628062057 | total_episodes = 1007 | fit/surrogate_loss = 0.1955440193414688 | fit/entropy_loss = 0.6441108584403992 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 124857 | episode_rewards = 0.0 | total_episodes = 1017 | fit/surrogate_loss = -3.1544911861419678 | fit/entropy_loss = 0.666680097579956 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 124857 | episode_rewards = 0.0 | total_episodes = 1017 | fit/surrogate_loss = -3.1544911861419678 | fit/entropy_loss = 0.666680097579956 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 125781 | episode_rewards = 0.0 | total_episodes = 1026 | fit/surrogate_loss = 2.1872572898864746 | fit/entropy_loss = 0.6689969301223755 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 125781 | episode_rewards = 0.0 | total_episodes = 1026 | fit/surrogate_loss = 2.1872572898864746 | fit/entropy_loss = 0.6689969301223755 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 126752 | episode_rewards = 355.32186690778263 | total_episodes = 1033 | fit/surrogate_loss = -4.678262233734131 | fit/entropy_loss = 0.7359442710876465 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 126752 | episode_rewards = 355.32186690778263 | total_episodes = 1033 | fit/surrogate_loss = -4.678262233734131 | fit/entropy_loss = 0.7359442710876465 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 127889 | episode_rewards = 0.0 | total_episodes = 1042 | fit/surrogate_loss = 7.985017776489258 | fit/entropy_loss = 0.6646859645843506 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 127889 | episode_rewards = 0.0 | total_episodes = 1042 | fit/surrogate_loss = 7.985017776489258 | fit/entropy_loss = 0.6646859645843506 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 128946 | episode_rewards = 0.0 | total_episodes = 1050 | fit/surrogate_loss = -1.8706543445587158 | fit/entropy_loss = 0.6729332804679871 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 128946 | episode_rewards = 0.0 | total_episodes = 1050 | fit/surrogate_loss = -1.8706543445587158 | fit/entropy_loss = 0.6729332804679871 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 129920 | episode_rewards = 0.0 | total_episodes = 1058 | fit/surrogate_loss = -1.8706543445587158 | fit/entropy_loss = 0.6729332804679871 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 129920 | episode_rewards = 0.0 | total_episodes = 1058 | fit/surrogate_loss = -1.8706543445587158 | fit/entropy_loss = 0.6729332804679871 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 130849 | episode_rewards = 0.0 | total_episodes = 1066 | fit/surrogate_loss = -3.248701810836792 | fit/entropy_loss = 0.7094629406929016 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 130849 | episode_rewards = 0.0 | total_episodes = 1066 | fit/surrogate_loss = -3.248701810836792 | fit/entropy_loss = 0.7094629406929016 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 131942 | episode_rewards = 0.0 | total_episodes = 1075 | fit/surrogate_loss = -5.871006965637207 | fit/entropy_loss = 0.7212318181991577 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 131942 | episode_rewards = 0.0 | total_episodes = 1075 | fit/surrogate_loss = -5.871006965637207 | fit/entropy_loss = 0.7212318181991577 | \n",
      "\u001b[38;21m[INFO] 18:22: [PPOAgent[worker: 0]] | max_global_step = 133086 | episode_rewards = 367.8032093389542 | total_episodes = 1085 | fit/surrogate_loss = -6.275293350219727 | fit/entropy_loss = 0.736537754535675 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 133086 | episode_rewards = 367.8032093389542 | total_episodes = 1085 | fit/surrogate_loss = -6.275293350219727 | fit/entropy_loss = 0.736537754535675 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 134037 | episode_rewards = 0.0 | total_episodes = 1093 | fit/surrogate_loss = 6.8658647537231445 | fit/entropy_loss = 0.7067789435386658 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 134037 | episode_rewards = 0.0 | total_episodes = 1093 | fit/surrogate_loss = 6.8658647537231445 | fit/entropy_loss = 0.7067789435386658 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 134978 | episode_rewards = 23.218850643478245 | total_episodes = 1100 | fit/surrogate_loss = 6.8658647537231445 | fit/entropy_loss = 0.7067789435386658 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 134978 | episode_rewards = 23.218850643478245 | total_episodes = 1100 | fit/surrogate_loss = 6.8658647537231445 | fit/entropy_loss = 0.7067789435386658 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 136149 | episode_rewards = 0.0 | total_episodes = 1109 | fit/surrogate_loss = -4.9170823097229 | fit/entropy_loss = 0.673271894454956 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 136149 | episode_rewards = 0.0 | total_episodes = 1109 | fit/surrogate_loss = -4.9170823097229 | fit/entropy_loss = 0.673271894454956 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 137304 | episode_rewards = 0.0 | total_episodes = 1119 | fit/surrogate_loss = 0.3117314577102661 | fit/entropy_loss = 0.6184284687042236 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 137304 | episode_rewards = 0.0 | total_episodes = 1119 | fit/surrogate_loss = 0.3117314577102661 | fit/entropy_loss = 0.6184284687042236 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 138267 | episode_rewards = 107.62177322917799 | total_episodes = 1126 | fit/surrogate_loss = 5.9346795082092285 | fit/entropy_loss = 0.6740566492080688 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 138267 | episode_rewards = 107.62177322917799 | total_episodes = 1126 | fit/surrogate_loss = 5.9346795082092285 | fit/entropy_loss = 0.6740566492080688 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 139137 | episode_rewards = 0.0 | total_episodes = 1133 | fit/surrogate_loss = -5.496123313903809 | fit/entropy_loss = 0.6874229907989502 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 139137 | episode_rewards = 0.0 | total_episodes = 1133 | fit/surrogate_loss = -5.496123313903809 | fit/entropy_loss = 0.6874229907989502 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 140311 | episode_rewards = 0.0 | total_episodes = 1142 | fit/surrogate_loss = -5.082326889038086 | fit/entropy_loss = 0.7692741751670837 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 140311 | episode_rewards = 0.0 | total_episodes = 1142 | fit/surrogate_loss = -5.082326889038086 | fit/entropy_loss = 0.7692741751670837 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 141340 | episode_rewards = 0.0 | total_episodes = 1150 | fit/surrogate_loss = -4.944938659667969 | fit/entropy_loss = 0.7782270312309265 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 141340 | episode_rewards = 0.0 | total_episodes = 1150 | fit/surrogate_loss = -4.944938659667969 | fit/entropy_loss = 0.7782270312309265 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 142392 | episode_rewards = 121.34665972821372 | total_episodes = 1159 | fit/surrogate_loss = -4.944938659667969 | fit/entropy_loss = 0.7782270312309265 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 142392 | episode_rewards = 121.34665972821372 | total_episodes = 1159 | fit/surrogate_loss = -4.944938659667969 | fit/entropy_loss = 0.7782270312309265 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 143309 | episode_rewards = 0.0 | total_episodes = 1166 | fit/surrogate_loss = 4.560090065002441 | fit/entropy_loss = 0.7792547345161438 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 143309 | episode_rewards = 0.0 | total_episodes = 1166 | fit/surrogate_loss = 4.560090065002441 | fit/entropy_loss = 0.7792547345161438 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 144504 | episode_rewards = 0.0 | total_episodes = 1177 | fit/surrogate_loss = 5.459929943084717 | fit/entropy_loss = 0.7123932838439941 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 144504 | episode_rewards = 0.0 | total_episodes = 1177 | fit/surrogate_loss = 5.459929943084717 | fit/entropy_loss = 0.7123932838439941 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 145572 | episode_rewards = 0.0 | total_episodes = 1188 | fit/surrogate_loss = 7.129532337188721 | fit/entropy_loss = 0.7167243361473083 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 145572 | episode_rewards = 0.0 | total_episodes = 1188 | fit/surrogate_loss = 7.129532337188721 | fit/entropy_loss = 0.7167243361473083 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 146545 | episode_rewards = 0.0 | total_episodes = 1196 | fit/surrogate_loss = -0.05080322548747063 | fit/entropy_loss = 0.7350640892982483 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 146545 | episode_rewards = 0.0 | total_episodes = 1196 | fit/surrogate_loss = -0.05080322548747063 | fit/entropy_loss = 0.7350640892982483 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 147382 | episode_rewards = 0.0 | total_episodes = 1204 | fit/surrogate_loss = -1.5923629999160767 | fit/entropy_loss = 0.7350640892982483 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 147382 | episode_rewards = 0.0 | total_episodes = 1204 | fit/surrogate_loss = -1.5923629999160767 | fit/entropy_loss = 0.7350640892982483 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 148672 | episode_rewards = 0.0 | total_episodes = 1212 | fit/surrogate_loss = -1.5923629999160767 | fit/entropy_loss = 0.69366055727005 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 148672 | episode_rewards = 0.0 | total_episodes = 1212 | fit/surrogate_loss = -1.5923629999160767 | fit/entropy_loss = 0.69366055727005 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 149829 | episode_rewards = 0.0 | total_episodes = 1221 | fit/surrogate_loss = 6.521337032318115 | fit/entropy_loss = 0.6350867748260498 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 149829 | episode_rewards = 0.0 | total_episodes = 1221 | fit/surrogate_loss = 6.521337032318115 | fit/entropy_loss = 0.6350867748260498 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 150808 | episode_rewards = 0.0 | total_episodes = 1231 | fit/surrogate_loss = 1.1787973642349243 | fit/entropy_loss = 0.6946080923080444 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 150808 | episode_rewards = 0.0 | total_episodes = 1231 | fit/surrogate_loss = 1.1787973642349243 | fit/entropy_loss = 0.6946080923080444 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 151688 | episode_rewards = 0.0 | total_episodes = 1238 | fit/surrogate_loss = -6.338792324066162 | fit/entropy_loss = 0.7006877660751343 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 151688 | episode_rewards = 0.0 | total_episodes = 1238 | fit/surrogate_loss = -6.338792324066162 | fit/entropy_loss = 0.7006877660751343 | \n",
      "\u001b[38;21m[INFO] 18:23: [PPOAgent[worker: 0]] | max_global_step = 152725 | episode_rewards = 0.0 | total_episodes = 1246 | fit/surrogate_loss = -5.212271690368652 | fit/entropy_loss = 0.6289504766464233 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 152725 | episode_rewards = 0.0 | total_episodes = 1246 | fit/surrogate_loss = -5.212271690368652 | fit/entropy_loss = 0.6289504766464233 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 153863 | episode_rewards = 0.0 | total_episodes = 1256 | fit/surrogate_loss = 1.7179265022277832 | fit/entropy_loss = 0.7683647871017456 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 153863 | episode_rewards = 0.0 | total_episodes = 1256 | fit/surrogate_loss = 1.7179265022277832 | fit/entropy_loss = 0.7683647871017456 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 154966 | episode_rewards = 256.05709469996594 | total_episodes = 1264 | fit/surrogate_loss = 1.7179265022277832 | fit/entropy_loss = 0.7683647871017456 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 154966 | episode_rewards = 256.05709469996594 | total_episodes = 1264 | fit/surrogate_loss = 1.7179265022277832 | fit/entropy_loss = 0.7683647871017456 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 155724 | episode_rewards = 0.0 | total_episodes = 1270 | fit/surrogate_loss = 6.379274845123291 | fit/entropy_loss = 0.7164989113807678 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 155724 | episode_rewards = 0.0 | total_episodes = 1270 | fit/surrogate_loss = 6.379274845123291 | fit/entropy_loss = 0.7164989113807678 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 156848 | episode_rewards = 0.0 | total_episodes = 1279 | fit/surrogate_loss = -2.251899242401123 | fit/entropy_loss = 0.7021844983100891 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 156848 | episode_rewards = 0.0 | total_episodes = 1279 | fit/surrogate_loss = -2.251899242401123 | fit/entropy_loss = 0.7021844983100891 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 157937 | episode_rewards = 18.0 | total_episodes = 1288 | fit/surrogate_loss = 0.8425665497779846 | fit/entropy_loss = 0.755340576171875 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 157937 | episode_rewards = 18.0 | total_episodes = 1288 | fit/surrogate_loss = 0.8425665497779846 | fit/entropy_loss = 0.755340576171875 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 159054 | episode_rewards = 0.0 | total_episodes = 1297 | fit/surrogate_loss = 3.0708093643188477 | fit/entropy_loss = 0.744597315788269 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 159054 | episode_rewards = 0.0 | total_episodes = 1297 | fit/surrogate_loss = 3.0708093643188477 | fit/entropy_loss = 0.744597315788269 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 159849 | episode_rewards = 65.88298250367251 | total_episodes = 1304 | fit/surrogate_loss = 3.0708093643188477 | fit/entropy_loss = 0.744597315788269 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 159849 | episode_rewards = 65.88298250367251 | total_episodes = 1304 | fit/surrogate_loss = 3.0708093643188477 | fit/entropy_loss = 0.744597315788269 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 160944 | episode_rewards = 0.0 | total_episodes = 1313 | fit/surrogate_loss = -1.1263442039489746 | fit/entropy_loss = 0.7117775082588196 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 160944 | episode_rewards = 0.0 | total_episodes = 1313 | fit/surrogate_loss = -1.1263442039489746 | fit/entropy_loss = 0.7117775082588196 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 162076 | episode_rewards = 74.0275682616461 | total_episodes = 1322 | fit/surrogate_loss = 5.945767879486084 | fit/entropy_loss = 0.7353214621543884 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 162076 | episode_rewards = 74.0275682616461 | total_episodes = 1322 | fit/surrogate_loss = 5.945767879486084 | fit/entropy_loss = 0.7353214621543884 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 163203 | episode_rewards = 214.77448881874915 | total_episodes = 1331 | fit/surrogate_loss = -1.9172329902648926 | fit/entropy_loss = 0.7719120383262634 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 163203 | episode_rewards = 214.77448881874915 | total_episodes = 1331 | fit/surrogate_loss = -1.9172329902648926 | fit/entropy_loss = 0.7719120383262634 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 164014 | episode_rewards = 0.0 | total_episodes = 1337 | fit/surrogate_loss = 4.019347667694092 | fit/entropy_loss = 0.6222264766693115 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 164014 | episode_rewards = 0.0 | total_episodes = 1337 | fit/surrogate_loss = 4.019347667694092 | fit/entropy_loss = 0.6222264766693115 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 165063 | episode_rewards = 0.0 | total_episodes = 1345 | fit/surrogate_loss = 5.211542129516602 | fit/entropy_loss = 0.648104190826416 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 165063 | episode_rewards = 0.0 | total_episodes = 1345 | fit/surrogate_loss = 5.211542129516602 | fit/entropy_loss = 0.648104190826416 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 166193 | episode_rewards = 23.700236383291063 | total_episodes = 1355 | fit/surrogate_loss = 5.211542129516602 | fit/entropy_loss = 0.648104190826416 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 166193 | episode_rewards = 23.700236383291063 | total_episodes = 1355 | fit/surrogate_loss = 5.211542129516602 | fit/entropy_loss = 0.648104190826416 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 167264 | episode_rewards = 0.0 | total_episodes = 1364 | fit/surrogate_loss = -3.816908359527588 | fit/entropy_loss = 0.6358683705329895 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 167264 | episode_rewards = 0.0 | total_episodes = 1364 | fit/surrogate_loss = -3.816908359527588 | fit/entropy_loss = 0.6358683705329895 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 168041 | episode_rewards = 0.0 | total_episodes = 1370 | fit/surrogate_loss = -4.720207214355469 | fit/entropy_loss = 0.6145480275154114 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 168041 | episode_rewards = 0.0 | total_episodes = 1370 | fit/surrogate_loss = -4.720207214355469 | fit/entropy_loss = 0.6145480275154114 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 169131 | episode_rewards = 354.6575570779366 | total_episodes = 1378 | fit/surrogate_loss = -6.889465808868408 | fit/entropy_loss = 0.6978411078453064 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 169131 | episode_rewards = 354.6575570779366 | total_episodes = 1378 | fit/surrogate_loss = -6.889465808868408 | fit/entropy_loss = 0.6978411078453064 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 170237 | episode_rewards = 0.0 | total_episodes = 1388 | fit/surrogate_loss = 3.179220199584961 | fit/entropy_loss = 0.7058930993080139 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 170237 | episode_rewards = 0.0 | total_episodes = 1388 | fit/surrogate_loss = 3.179220199584961 | fit/entropy_loss = 0.7058930993080139 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 171436 | episode_rewards = 0.0 | total_episodes = 1397 | fit/surrogate_loss = 4.737096786499023 | fit/entropy_loss = 0.6763713955879211 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 171436 | episode_rewards = 0.0 | total_episodes = 1397 | fit/surrogate_loss = 4.737096786499023 | fit/entropy_loss = 0.6763713955879211 | \n",
      "\u001b[38;21m[INFO] 18:24: [PPOAgent[worker: 0]] | max_global_step = 172318 | episode_rewards = 168.902350770954 | total_episodes = 1404 | fit/surrogate_loss = 4.737096786499023 | fit/entropy_loss = 0.6763713955879211 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 172318 | episode_rewards = 168.902350770954 | total_episodes = 1404 | fit/surrogate_loss = 4.737096786499023 | fit/entropy_loss = 0.6763713955879211 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 173399 | episode_rewards = 0.0 | total_episodes = 1413 | fit/surrogate_loss = -0.6595741510391235 | fit/entropy_loss = 0.6700150370597839 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 173399 | episode_rewards = 0.0 | total_episodes = 1413 | fit/surrogate_loss = -0.6595741510391235 | fit/entropy_loss = 0.6700150370597839 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 174532 | episode_rewards = 19.838323782591793 | total_episodes = 1421 | fit/surrogate_loss = -4.190738201141357 | fit/entropy_loss = 0.6306287050247192 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 174532 | episode_rewards = 19.838323782591793 | total_episodes = 1421 | fit/surrogate_loss = -4.190738201141357 | fit/entropy_loss = 0.6306287050247192 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 175618 | episode_rewards = 41.351887773460774 | total_episodes = 1430 | fit/surrogate_loss = -6.189172267913818 | fit/entropy_loss = 0.7205357551574707 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 175618 | episode_rewards = 41.351887773460774 | total_episodes = 1430 | fit/surrogate_loss = -6.189172267913818 | fit/entropy_loss = 0.7205357551574707 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 176487 | episode_rewards = 0.0 | total_episodes = 1437 | fit/surrogate_loss = -6.361572742462158 | fit/entropy_loss = 0.7578191161155701 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 176487 | episode_rewards = 0.0 | total_episodes = 1437 | fit/surrogate_loss = -6.361572742462158 | fit/entropy_loss = 0.7578191161155701 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 177589 | episode_rewards = 17.0 | total_episodes = 1447 | fit/surrogate_loss = -1.1256687641143799 | fit/entropy_loss = 0.767536997795105 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 177589 | episode_rewards = 17.0 | total_episodes = 1447 | fit/surrogate_loss = -1.1256687641143799 | fit/entropy_loss = 0.767536997795105 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 178674 | episode_rewards = 0.0 | total_episodes = 1457 | fit/surrogate_loss = -2.159406900405884 | fit/entropy_loss = 0.767536997795105 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 178674 | episode_rewards = 0.0 | total_episodes = 1457 | fit/surrogate_loss = -2.159406900405884 | fit/entropy_loss = 0.767536997795105 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 179983 | episode_rewards = 542.1157222061416 | total_episodes = 1467 | fit/surrogate_loss = -2.159406900405884 | fit/entropy_loss = 0.8187519907951355 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 179983 | episode_rewards = 542.1157222061416 | total_episodes = 1467 | fit/surrogate_loss = -2.159406900405884 | fit/entropy_loss = 0.8187519907951355 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 180725 | episode_rewards = 0.0 | total_episodes = 1474 | fit/surrogate_loss = 10.045548439025879 | fit/entropy_loss = 0.816155195236206 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 180725 | episode_rewards = 0.0 | total_episodes = 1474 | fit/surrogate_loss = 10.045548439025879 | fit/entropy_loss = 0.816155195236206 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 181736 | episode_rewards = 0.0 | total_episodes = 1482 | fit/surrogate_loss = 25.3927059173584 | fit/entropy_loss = 0.7565316557884216 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 181736 | episode_rewards = 0.0 | total_episodes = 1482 | fit/surrogate_loss = 25.3927059173584 | fit/entropy_loss = 0.7565316557884216 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 182846 | episode_rewards = 358.544885067706 | total_episodes = 1492 | fit/surrogate_loss = -7.172727108001709 | fit/entropy_loss = 0.7205567359924316 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 182846 | episode_rewards = 358.544885067706 | total_episodes = 1492 | fit/surrogate_loss = -7.172727108001709 | fit/entropy_loss = 0.7205567359924316 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 183918 | episode_rewards = 186.5764862125093 | total_episodes = 1501 | fit/surrogate_loss = 5.249162197113037 | fit/entropy_loss = 0.8371127247810364 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 183918 | episode_rewards = 186.5764862125093 | total_episodes = 1501 | fit/surrogate_loss = 5.249162197113037 | fit/entropy_loss = 0.8371127247810364 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 184817 | episode_rewards = 0.0 | total_episodes = 1509 | fit/surrogate_loss = 5.249162197113037 | fit/entropy_loss = 0.8371127247810364 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 184817 | episode_rewards = 0.0 | total_episodes = 1509 | fit/surrogate_loss = 5.249162197113037 | fit/entropy_loss = 0.8371127247810364 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 185778 | episode_rewards = 0.0 | total_episodes = 1517 | fit/surrogate_loss = 4.2216010093688965 | fit/entropy_loss = 0.762433648109436 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 185778 | episode_rewards = 0.0 | total_episodes = 1517 | fit/surrogate_loss = 4.2216010093688965 | fit/entropy_loss = 0.762433648109436 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 186883 | episode_rewards = 116.57339660400876 | total_episodes = 1526 | fit/surrogate_loss = 4.216997146606445 | fit/entropy_loss = 0.693997323513031 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 186883 | episode_rewards = 116.57339660400876 | total_episodes = 1526 | fit/surrogate_loss = 4.216997146606445 | fit/entropy_loss = 0.693997323513031 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 187880 | episode_rewards = 40.5527805040477 | total_episodes = 1534 | fit/surrogate_loss = 1.3277157545089722 | fit/entropy_loss = 0.7891522645950317 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 187880 | episode_rewards = 40.5527805040477 | total_episodes = 1534 | fit/surrogate_loss = 1.3277157545089722 | fit/entropy_loss = 0.7891522645950317 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 188742 | episode_rewards = 0.0 | total_episodes = 1543 | fit/surrogate_loss = 1.6497738361358643 | fit/entropy_loss = 0.7891522645950317 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 188742 | episode_rewards = 0.0 | total_episodes = 1543 | fit/surrogate_loss = 1.6497738361358643 | fit/entropy_loss = 0.7891522645950317 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 189716 | episode_rewards = 0.0 | total_episodes = 1550 | fit/surrogate_loss = 1.6497738361358643 | fit/entropy_loss = 0.7597564458847046 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 189716 | episode_rewards = 0.0 | total_episodes = 1550 | fit/surrogate_loss = 1.6497738361358643 | fit/entropy_loss = 0.7597564458847046 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 190868 | episode_rewards = 614.474217456186 | total_episodes = 1559 | fit/surrogate_loss = 10.002662658691406 | fit/entropy_loss = 0.7268947958946228 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 190868 | episode_rewards = 614.474217456186 | total_episodes = 1559 | fit/surrogate_loss = 10.002662658691406 | fit/entropy_loss = 0.7268947958946228 | \n",
      "\u001b[38;21m[INFO] 18:25: [PPOAgent[worker: 0]] | max_global_step = 191942 | episode_rewards = 31.45886523347028 | total_episodes = 1569 | fit/surrogate_loss = 15.501335144042969 | fit/entropy_loss = 0.7379599809646606 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 191942 | episode_rewards = 31.45886523347028 | total_episodes = 1569 | fit/surrogate_loss = 15.501335144042969 | fit/entropy_loss = 0.7379599809646606 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 192884 | episode_rewards = 547.4287525706364 | total_episodes = 1577 | fit/surrogate_loss = -11.351655006408691 | fit/entropy_loss = 0.7981424927711487 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 192884 | episode_rewards = 547.4287525706364 | total_episodes = 1577 | fit/surrogate_loss = -11.351655006408691 | fit/entropy_loss = 0.7981424927711487 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 193737 | episode_rewards = 395.5573231066538 | total_episodes = 1584 | fit/surrogate_loss = -11.351655006408691 | fit/entropy_loss = 0.7981424927711487 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 193737 | episode_rewards = 395.5573231066538 | total_episodes = 1584 | fit/surrogate_loss = -11.351655006408691 | fit/entropy_loss = 0.7981424927711487 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 194894 | episode_rewards = 170.89198960106714 | total_episodes = 1596 | fit/surrogate_loss = 16.778554916381836 | fit/entropy_loss = 0.7176771759986877 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 194894 | episode_rewards = 170.89198960106714 | total_episodes = 1596 | fit/surrogate_loss = 16.778554916381836 | fit/entropy_loss = 0.7176771759986877 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 195966 | episode_rewards = 0.0 | total_episodes = 1606 | fit/surrogate_loss = -0.2552869915962219 | fit/entropy_loss = 0.783818781375885 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 195966 | episode_rewards = 0.0 | total_episodes = 1606 | fit/surrogate_loss = -0.2552869915962219 | fit/entropy_loss = 0.783818781375885 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 197074 | episode_rewards = 190.71231975681135 | total_episodes = 1616 | fit/surrogate_loss = -4.0645751953125 | fit/entropy_loss = 0.6498353481292725 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 197074 | episode_rewards = 190.71231975681135 | total_episodes = 1616 | fit/surrogate_loss = -4.0645751953125 | fit/entropy_loss = 0.6498353481292725 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 198037 | episode_rewards = 179.04767936622915 | total_episodes = 1625 | fit/surrogate_loss = 0.9871432185173035 | fit/entropy_loss = 0.7556384205818176 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 198037 | episode_rewards = 179.04767936622915 | total_episodes = 1625 | fit/surrogate_loss = 0.9871432185173035 | fit/entropy_loss = 0.7556384205818176 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 199189 | episode_rewards = 273.1383223699719 | total_episodes = 1635 | fit/surrogate_loss = -4.294948101043701 | fit/entropy_loss = 0.6763026714324951 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 199189 | episode_rewards = 273.1383223699719 | total_episodes = 1635 | fit/surrogate_loss = -4.294948101043701 | fit/entropy_loss = 0.6763026714324951 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 200257 | episode_rewards = 0.0 | total_episodes = 1645 | fit/surrogate_loss = 1.460858702659607 | fit/entropy_loss = 0.745467483997345 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 200257 | episode_rewards = 0.0 | total_episodes = 1645 | fit/surrogate_loss = 1.460858702659607 | fit/entropy_loss = 0.745467483997345 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 201185 | episode_rewards = 0.0 | total_episodes = 1654 | fit/surrogate_loss = -13.150717735290527 | fit/entropy_loss = 0.745467483997345 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 201185 | episode_rewards = 0.0 | total_episodes = 1654 | fit/surrogate_loss = -13.150717735290527 | fit/entropy_loss = 0.745467483997345 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 202149 | episode_rewards = 0.0 | total_episodes = 1663 | fit/surrogate_loss = -13.150717735290527 | fit/entropy_loss = 0.7357959747314453 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 202149 | episode_rewards = 0.0 | total_episodes = 1663 | fit/surrogate_loss = -13.150717735290527 | fit/entropy_loss = 0.7357959747314453 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 203284 | episode_rewards = 0.0 | total_episodes = 1674 | fit/surrogate_loss = -5.055821418762207 | fit/entropy_loss = 0.7882977724075317 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 203284 | episode_rewards = 0.0 | total_episodes = 1674 | fit/surrogate_loss = -5.055821418762207 | fit/entropy_loss = 0.7882977724075317 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 204421 | episode_rewards = 0.0 | total_episodes = 1684 | fit/surrogate_loss = 3.581916332244873 | fit/entropy_loss = 0.8109924793243408 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 204421 | episode_rewards = 0.0 | total_episodes = 1684 | fit/surrogate_loss = 3.581916332244873 | fit/entropy_loss = 0.8109924793243408 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 205431 | episode_rewards = 0.0 | total_episodes = 1694 | fit/surrogate_loss = 3.8135762214660645 | fit/entropy_loss = 0.7934265732765198 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 205431 | episode_rewards = 0.0 | total_episodes = 1694 | fit/surrogate_loss = 3.8135762214660645 | fit/entropy_loss = 0.7934265732765198 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 206170 | episode_rewards = 0.0 | total_episodes = 1703 | fit/surrogate_loss = 7.423683166503906 | fit/entropy_loss = 0.7934265732765198 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 206170 | episode_rewards = 0.0 | total_episodes = 1703 | fit/surrogate_loss = 7.423683166503906 | fit/entropy_loss = 0.7934265732765198 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 207405 | episode_rewards = 211.55534728795374 | total_episodes = 1714 | fit/surrogate_loss = 7.423683166503906 | fit/entropy_loss = 0.8412510752677917 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 207405 | episode_rewards = 211.55534728795374 | total_episodes = 1714 | fit/surrogate_loss = 7.423683166503906 | fit/entropy_loss = 0.8412510752677917 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 208552 | episode_rewards = 0.0 | total_episodes = 1725 | fit/surrogate_loss = 9.417638778686523 | fit/entropy_loss = 0.8021760582923889 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 208552 | episode_rewards = 0.0 | total_episodes = 1725 | fit/surrogate_loss = 9.417638778686523 | fit/entropy_loss = 0.8021760582923889 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 209624 | episode_rewards = 0.0 | total_episodes = 1735 | fit/surrogate_loss = 2.8593873977661133 | fit/entropy_loss = 0.8252291679382324 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 209624 | episode_rewards = 0.0 | total_episodes = 1735 | fit/surrogate_loss = 2.8593873977661133 | fit/entropy_loss = 0.8252291679382324 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 210444 | episode_rewards = 255.6599719163215 | total_episodes = 1742 | fit/surrogate_loss = 10.403597831726074 | fit/entropy_loss = 0.742159366607666 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 210444 | episode_rewards = 255.6599719163215 | total_episodes = 1742 | fit/surrogate_loss = 10.403597831726074 | fit/entropy_loss = 0.742159366607666 | \n",
      "\u001b[38;21m[INFO] 18:26: [PPOAgent[worker: 0]] | max_global_step = 211469 | episode_rewards = 0.0 | total_episodes = 1752 | fit/surrogate_loss = 5.156689643859863 | fit/entropy_loss = 0.7956405282020569 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 211469 | episode_rewards = 0.0 | total_episodes = 1752 | fit/surrogate_loss = 5.156689643859863 | fit/entropy_loss = 0.7956405282020569 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 212586 | episode_rewards = 18.0 | total_episodes = 1765 | fit/surrogate_loss = -11.282405853271484 | fit/entropy_loss = 0.7843111753463745 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 212586 | episode_rewards = 18.0 | total_episodes = 1765 | fit/surrogate_loss = -11.282405853271484 | fit/entropy_loss = 0.7843111753463745 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 213430 | episode_rewards = 18.0 | total_episodes = 1773 | fit/surrogate_loss = -11.282405853271484 | fit/entropy_loss = 0.7843111753463745 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 213430 | episode_rewards = 18.0 | total_episodes = 1773 | fit/surrogate_loss = -11.282405853271484 | fit/entropy_loss = 0.7843111753463745 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 214167 | episode_rewards = 165.14612018361703 | total_episodes = 1781 | fit/surrogate_loss = -5.131540775299072 | fit/entropy_loss = 0.7984173893928528 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 214167 | episode_rewards = 165.14612018361703 | total_episodes = 1781 | fit/surrogate_loss = -5.131540775299072 | fit/entropy_loss = 0.7984173893928528 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 215266 | episode_rewards = 0.0 | total_episodes = 1792 | fit/surrogate_loss = -10.625511169433594 | fit/entropy_loss = 0.8293630480766296 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 215266 | episode_rewards = 0.0 | total_episodes = 1792 | fit/surrogate_loss = -10.625511169433594 | fit/entropy_loss = 0.8293630480766296 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 216389 | episode_rewards = 252.14652381800485 | total_episodes = 1803 | fit/surrogate_loss = 5.982193470001221 | fit/entropy_loss = 0.8315790891647339 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 216389 | episode_rewards = 252.14652381800485 | total_episodes = 1803 | fit/surrogate_loss = 5.982193470001221 | fit/entropy_loss = 0.8315790891647339 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 217454 | episode_rewards = 127.91840482282323 | total_episodes = 1814 | fit/surrogate_loss = -5.009496212005615 | fit/entropy_loss = 0.8315790891647339 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 217454 | episode_rewards = 127.91840482282323 | total_episodes = 1814 | fit/surrogate_loss = -5.009496212005615 | fit/entropy_loss = 0.8315790891647339 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 218239 | episode_rewards = 739.2529412311961 | total_episodes = 1821 | fit/surrogate_loss = -5.009496212005615 | fit/entropy_loss = 0.8159546256065369 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 218239 | episode_rewards = 739.2529412311961 | total_episodes = 1821 | fit/surrogate_loss = -5.009496212005615 | fit/entropy_loss = 0.8159546256065369 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 219279 | episode_rewards = 0.0 | total_episodes = 1831 | fit/surrogate_loss = 14.628860473632812 | fit/entropy_loss = 0.702031672000885 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 219279 | episode_rewards = 0.0 | total_episodes = 1831 | fit/surrogate_loss = 14.628860473632812 | fit/entropy_loss = 0.702031672000885 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 220381 | episode_rewards = 261.9296411464575 | total_episodes = 1842 | fit/surrogate_loss = -1.7341861724853516 | fit/entropy_loss = 0.8232353925704956 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 220381 | episode_rewards = 261.9296411464575 | total_episodes = 1842 | fit/surrogate_loss = -1.7341861724853516 | fit/entropy_loss = 0.8232353925704956 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 221456 | episode_rewards = 18.0 | total_episodes = 1852 | fit/surrogate_loss = 0.3010912537574768 | fit/entropy_loss = 0.8722078800201416 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 221456 | episode_rewards = 18.0 | total_episodes = 1852 | fit/surrogate_loss = 0.3010912537574768 | fit/entropy_loss = 0.8722078800201416 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 222314 | episode_rewards = 30.188212645388766 | total_episodes = 1860 | fit/surrogate_loss = 0.3010912537574768 | fit/entropy_loss = 0.8722078800201416 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 222314 | episode_rewards = 30.188212645388766 | total_episodes = 1860 | fit/surrogate_loss = 0.3010912537574768 | fit/entropy_loss = 0.8722078800201416 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 223247 | episode_rewards = 0.0 | total_episodes = 1869 | fit/surrogate_loss = -7.583054065704346 | fit/entropy_loss = 0.7252102494239807 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 223247 | episode_rewards = 0.0 | total_episodes = 1869 | fit/surrogate_loss = -7.583054065704346 | fit/entropy_loss = 0.7252102494239807 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 224344 | episode_rewards = 602.5976856828167 | total_episodes = 1879 | fit/surrogate_loss = -0.7887477874755859 | fit/entropy_loss = 0.7979558706283569 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 224344 | episode_rewards = 602.5976856828167 | total_episodes = 1879 | fit/surrogate_loss = -0.7887477874755859 | fit/entropy_loss = 0.7979558706283569 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 225419 | episode_rewards = 18.0 | total_episodes = 1890 | fit/surrogate_loss = -3.086843490600586 | fit/entropy_loss = 0.786424458026886 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 225419 | episode_rewards = 18.0 | total_episodes = 1890 | fit/surrogate_loss = -3.086843490600586 | fit/entropy_loss = 0.786424458026886 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 226314 | episode_rewards = 303.8208309128256 | total_episodes = 1899 | fit/surrogate_loss = 16.572532653808594 | fit/entropy_loss = 0.7617034912109375 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 226314 | episode_rewards = 303.8208309128256 | total_episodes = 1899 | fit/surrogate_loss = 16.572532653808594 | fit/entropy_loss = 0.7617034912109375 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 227379 | episode_rewards = 86.3244040057659 | total_episodes = 1909 | fit/surrogate_loss = 16.572532653808594 | fit/entropy_loss = 0.7617034912109375 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 227379 | episode_rewards = 86.3244040057659 | total_episodes = 1909 | fit/surrogate_loss = 16.572532653808594 | fit/entropy_loss = 0.7617034912109375 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 228457 | episode_rewards = 136.32565857666347 | total_episodes = 1919 | fit/surrogate_loss = -3.1739420890808105 | fit/entropy_loss = 0.7872847318649292 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 228457 | episode_rewards = 136.32565857666347 | total_episodes = 1919 | fit/surrogate_loss = -3.1739420890808105 | fit/entropy_loss = 0.7872847318649292 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 229543 | episode_rewards = 0.0 | total_episodes = 1929 | fit/surrogate_loss = 22.5654354095459 | fit/entropy_loss = 0.641607403755188 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 229543 | episode_rewards = 0.0 | total_episodes = 1929 | fit/surrogate_loss = 22.5654354095459 | fit/entropy_loss = 0.641607403755188 | \n",
      "\u001b[38;21m[INFO] 18:27: [PPOAgent[worker: 0]] | max_global_step = 230456 | episode_rewards = 84.85943601706278 | total_episodes = 1938 | fit/surrogate_loss = -9.793035507202148 | fit/entropy_loss = 0.835252046585083 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 230456 | episode_rewards = 84.85943601706278 | total_episodes = 1938 | fit/surrogate_loss = -9.793035507202148 | fit/entropy_loss = 0.835252046585083 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 231466 | episode_rewards = 0.0 | total_episodes = 1947 | fit/surrogate_loss = 4.8574538230896 | fit/entropy_loss = 0.8109703660011292 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 231466 | episode_rewards = 0.0 | total_episodes = 1947 | fit/surrogate_loss = 4.8574538230896 | fit/entropy_loss = 0.8109703660011292 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 232608 | episode_rewards = 0.0 | total_episodes = 1958 | fit/surrogate_loss = 2.133427619934082 | fit/entropy_loss = 0.8220601081848145 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 232608 | episode_rewards = 0.0 | total_episodes = 1958 | fit/surrogate_loss = 2.133427619934082 | fit/entropy_loss = 0.8220601081848145 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 233686 | episode_rewards = 0.0 | total_episodes = 1969 | fit/surrogate_loss = 5.453704357147217 | fit/entropy_loss = 0.8220601081848145 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 233686 | episode_rewards = 0.0 | total_episodes = 1969 | fit/surrogate_loss = 5.453704357147217 | fit/entropy_loss = 0.8220601081848145 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 234721 | episode_rewards = 0.0 | total_episodes = 1977 | fit/surrogate_loss = 5.453704357147217 | fit/entropy_loss = 0.7349075078964233 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 234721 | episode_rewards = 0.0 | total_episodes = 1977 | fit/surrogate_loss = 5.453704357147217 | fit/entropy_loss = 0.7349075078964233 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 235634 | episode_rewards = 0.0 | total_episodes = 1986 | fit/surrogate_loss = -0.4515523910522461 | fit/entropy_loss = 0.7926269769668579 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 235634 | episode_rewards = 0.0 | total_episodes = 1986 | fit/surrogate_loss = -0.4515523910522461 | fit/entropy_loss = 0.7926269769668579 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 236766 | episode_rewards = 0.0 | total_episodes = 1996 | fit/surrogate_loss = 10.071703910827637 | fit/entropy_loss = 0.6521742343902588 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 236766 | episode_rewards = 0.0 | total_episodes = 1996 | fit/surrogate_loss = 10.071703910827637 | fit/entropy_loss = 0.6521742343902588 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 237908 | episode_rewards = 0.0 | total_episodes = 2007 | fit/surrogate_loss = -2.5159590244293213 | fit/entropy_loss = 0.8044253587722778 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 237908 | episode_rewards = 0.0 | total_episodes = 2007 | fit/surrogate_loss = -2.5159590244293213 | fit/entropy_loss = 0.8044253587722778 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 238816 | episode_rewards = 88.15079687127205 | total_episodes = 2017 | fit/surrogate_loss = -9.821616172790527 | fit/entropy_loss = 0.7048015594482422 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 238816 | episode_rewards = 88.15079687127205 | total_episodes = 2017 | fit/surrogate_loss = -9.821616172790527 | fit/entropy_loss = 0.7048015594482422 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 239810 | episode_rewards = 0.0 | total_episodes = 2028 | fit/surrogate_loss = -9.821616172790527 | fit/entropy_loss = 0.7048015594482422 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 239810 | episode_rewards = 0.0 | total_episodes = 2028 | fit/surrogate_loss = -9.821616172790527 | fit/entropy_loss = 0.7048015594482422 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 240838 | episode_rewards = 0.0 | total_episodes = 2038 | fit/surrogate_loss = -3.4976515769958496 | fit/entropy_loss = 0.7336905598640442 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 240838 | episode_rewards = 0.0 | total_episodes = 2038 | fit/surrogate_loss = -3.4976515769958496 | fit/entropy_loss = 0.7336905598640442 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 241927 | episode_rewards = 0.0 | total_episodes = 2049 | fit/surrogate_loss = -7.736522674560547 | fit/entropy_loss = 0.6838456392288208 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 241927 | episode_rewards = 0.0 | total_episodes = 2049 | fit/surrogate_loss = -7.736522674560547 | fit/entropy_loss = 0.6838456392288208 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 242918 | episode_rewards = 18.0 | total_episodes = 2058 | fit/surrogate_loss = -13.8356294631958 | fit/entropy_loss = 0.70811527967453 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 242918 | episode_rewards = 18.0 | total_episodes = 2058 | fit/surrogate_loss = -13.8356294631958 | fit/entropy_loss = 0.70811527967453 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 243651 | episode_rewards = 69.35196836504993 | total_episodes = 2067 | fit/surrogate_loss = -6.078245639801025 | fit/entropy_loss = 0.70811527967453 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 243651 | episode_rewards = 69.35196836504993 | total_episodes = 2067 | fit/surrogate_loss = -6.078245639801025 | fit/entropy_loss = 0.70811527967453 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 244933 | episode_rewards = 0.0 | total_episodes = 2078 | fit/surrogate_loss = -6.078245639801025 | fit/entropy_loss = 0.8075422644615173 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 244933 | episode_rewards = 0.0 | total_episodes = 2078 | fit/surrogate_loss = -6.078245639801025 | fit/entropy_loss = 0.8075422644615173 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 246052 | episode_rewards = 591.4551296612558 | total_episodes = 2088 | fit/surrogate_loss = -6.30687952041626 | fit/entropy_loss = 0.6890563368797302 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 246052 | episode_rewards = 591.4551296612558 | total_episodes = 2088 | fit/surrogate_loss = -6.30687952041626 | fit/entropy_loss = 0.6890563368797302 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 247107 | episode_rewards = 0.0 | total_episodes = 2098 | fit/surrogate_loss = 12.747772216796875 | fit/entropy_loss = 0.7017247676849365 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 247107 | episode_rewards = 0.0 | total_episodes = 2098 | fit/surrogate_loss = 12.747772216796875 | fit/entropy_loss = 0.7017247676849365 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 247956 | episode_rewards = 537.9273792778669 | total_episodes = 2106 | fit/surrogate_loss = -8.093117713928223 | fit/entropy_loss = 0.6439991593360901 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 247956 | episode_rewards = 537.9273792778669 | total_episodes = 2106 | fit/surrogate_loss = -8.093117713928223 | fit/entropy_loss = 0.6439991593360901 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 249055 | episode_rewards = 0.0 | total_episodes = 2116 | fit/surrogate_loss = 7.664409637451172 | fit/entropy_loss = 0.7630298137664795 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 249055 | episode_rewards = 0.0 | total_episodes = 2116 | fit/surrogate_loss = 7.664409637451172 | fit/entropy_loss = 0.7630298137664795 | \n",
      "\u001b[38;21m[INFO] 18:28: [PPOAgent[worker: 0]] | max_global_step = 250140 | episode_rewards = 0.0 | total_episodes = 2126 | fit/surrogate_loss = -3.4463579654693604 | fit/entropy_loss = 0.769597053527832 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 250140 | episode_rewards = 0.0 | total_episodes = 2126 | fit/surrogate_loss = -3.4463579654693604 | fit/entropy_loss = 0.769597053527832 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 251150 | episode_rewards = 202.19817383593144 | total_episodes = 2137 | fit/surrogate_loss = 16.20615577697754 | fit/entropy_loss = 0.769597053527832 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 251150 | episode_rewards = 202.19817383593144 | total_episodes = 2137 | fit/surrogate_loss = 16.20615577697754 | fit/entropy_loss = 0.769597053527832 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 252124 | episode_rewards = 0.0 | total_episodes = 2145 | fit/surrogate_loss = 16.20615577697754 | fit/entropy_loss = 0.6610027551651001 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 252124 | episode_rewards = 0.0 | total_episodes = 2145 | fit/surrogate_loss = 16.20615577697754 | fit/entropy_loss = 0.6610027551651001 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 253271 | episode_rewards = 112.76136650619378 | total_episodes = 2156 | fit/surrogate_loss = -1.7488117218017578 | fit/entropy_loss = 0.6953644156455994 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 253271 | episode_rewards = 112.76136650619378 | total_episodes = 2156 | fit/surrogate_loss = -1.7488117218017578 | fit/entropy_loss = 0.6953644156455994 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 254362 | episode_rewards = 17.0 | total_episodes = 2166 | fit/surrogate_loss = 0.1633549928665161 | fit/entropy_loss = 0.6243811249732971 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 254362 | episode_rewards = 17.0 | total_episodes = 2166 | fit/surrogate_loss = 0.1633549928665161 | fit/entropy_loss = 0.6243811249732971 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 255367 | episode_rewards = 356.4956730494254 | total_episodes = 2175 | fit/surrogate_loss = 18.766645431518555 | fit/entropy_loss = 0.6704123020172119 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 255367 | episode_rewards = 356.4956730494254 | total_episodes = 2175 | fit/surrogate_loss = 18.766645431518555 | fit/entropy_loss = 0.6704123020172119 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 256204 | episode_rewards = 0.0 | total_episodes = 2184 | fit/surrogate_loss = 18.766645431518555 | fit/entropy_loss = 0.6704123020172119 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 256204 | episode_rewards = 0.0 | total_episodes = 2184 | fit/surrogate_loss = 18.766645431518555 | fit/entropy_loss = 0.6704123020172119 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 257307 | episode_rewards = 0.0 | total_episodes = 2194 | fit/surrogate_loss = 0.7888317108154297 | fit/entropy_loss = 0.7019331455230713 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 257307 | episode_rewards = 0.0 | total_episodes = 2194 | fit/surrogate_loss = 0.7888317108154297 | fit/entropy_loss = 0.7019331455230713 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 258343 | episode_rewards = 0.0 | total_episodes = 2203 | fit/surrogate_loss = 6.859912872314453 | fit/entropy_loss = 0.6788917779922485 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 258343 | episode_rewards = 0.0 | total_episodes = 2203 | fit/surrogate_loss = 6.859912872314453 | fit/entropy_loss = 0.6788917779922485 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 259389 | episode_rewards = 0.0 | total_episodes = 2214 | fit/surrogate_loss = -8.014985084533691 | fit/entropy_loss = 0.7212223410606384 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 259389 | episode_rewards = 0.0 | total_episodes = 2214 | fit/surrogate_loss = -8.014985084533691 | fit/entropy_loss = 0.7212223410606384 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 260125 | episode_rewards = 0.0 | total_episodes = 2221 | fit/surrogate_loss = -0.816467821598053 | fit/entropy_loss = 0.6649698615074158 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 260125 | episode_rewards = 0.0 | total_episodes = 2221 | fit/surrogate_loss = -0.816467821598053 | fit/entropy_loss = 0.6649698615074158 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 261186 | episode_rewards = 18.0 | total_episodes = 2232 | fit/surrogate_loss = -6.884901523590088 | fit/entropy_loss = 0.6649698615074158 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 261186 | episode_rewards = 18.0 | total_episodes = 2232 | fit/surrogate_loss = -6.884901523590088 | fit/entropy_loss = 0.6649698615074158 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 262437 | episode_rewards = 219.42548240088578 | total_episodes = 2242 | fit/surrogate_loss = -6.884901523590088 | fit/entropy_loss = 0.6594785451889038 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 262437 | episode_rewards = 219.42548240088578 | total_episodes = 2242 | fit/surrogate_loss = -6.884901523590088 | fit/entropy_loss = 0.6594785451889038 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 263528 | episode_rewards = 0.0 | total_episodes = 2252 | fit/surrogate_loss = 8.562251091003418 | fit/entropy_loss = 0.6798112392425537 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 263528 | episode_rewards = 0.0 | total_episodes = 2252 | fit/surrogate_loss = 8.562251091003418 | fit/entropy_loss = 0.6798112392425537 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 264190 | episode_rewards = 0.0 | total_episodes = 2258 | fit/surrogate_loss = -0.7726454734802246 | fit/entropy_loss = 0.7732981443405151 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 264190 | episode_rewards = 0.0 | total_episodes = 2258 | fit/surrogate_loss = -0.7726454734802246 | fit/entropy_loss = 0.7732981443405151 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 265254 | episode_rewards = 31.670137776990536 | total_episodes = 2268 | fit/surrogate_loss = 5.299010276794434 | fit/entropy_loss = 0.6774266362190247 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 265254 | episode_rewards = 31.670137776990536 | total_episodes = 2268 | fit/surrogate_loss = 5.299010276794434 | fit/entropy_loss = 0.6774266362190247 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 266351 | episode_rewards = 117.02427732993354 | total_episodes = 2278 | fit/surrogate_loss = -0.24698585271835327 | fit/entropy_loss = 0.7011128664016724 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 266351 | episode_rewards = 117.02427732993354 | total_episodes = 2278 | fit/surrogate_loss = -0.24698585271835327 | fit/entropy_loss = 0.7011128664016724 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 267427 | episode_rewards = 237.74798453635364 | total_episodes = 2289 | fit/surrogate_loss = -0.5983991622924805 | fit/entropy_loss = 0.7011128664016724 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 267427 | episode_rewards = 237.74798453635364 | total_episodes = 2289 | fit/surrogate_loss = -0.5983991622924805 | fit/entropy_loss = 0.7011128664016724 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 268441 | episode_rewards = 0.0 | total_episodes = 2298 | fit/surrogate_loss = -0.5983991622924805 | fit/entropy_loss = 0.7451872229576111 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 268441 | episode_rewards = 0.0 | total_episodes = 2298 | fit/surrogate_loss = -0.5983991622924805 | fit/entropy_loss = 0.7451872229576111 | \n",
      "\u001b[38;21m[INFO] 18:29: [PPOAgent[worker: 0]] | max_global_step = 269456 | episode_rewards = 0.0 | total_episodes = 2308 | fit/surrogate_loss = -0.354604035615921 | fit/entropy_loss = 0.6875908970832825 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 269456 | episode_rewards = 0.0 | total_episodes = 2308 | fit/surrogate_loss = -0.354604035615921 | fit/entropy_loss = 0.6875908970832825 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 270513 | episode_rewards = 43.78798783149697 | total_episodes = 2318 | fit/surrogate_loss = 7.654101848602295 | fit/entropy_loss = 0.7485741376876831 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 270513 | episode_rewards = 43.78798783149697 | total_episodes = 2318 | fit/surrogate_loss = 7.654101848602295 | fit/entropy_loss = 0.7485741376876831 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 271583 | episode_rewards = 408.69883716418803 | total_episodes = 2328 | fit/surrogate_loss = 10.600443840026855 | fit/entropy_loss = 0.6474515795707703 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 271583 | episode_rewards = 408.69883716418803 | total_episodes = 2328 | fit/surrogate_loss = 10.600443840026855 | fit/entropy_loss = 0.6474515795707703 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 272424 | episode_rewards = 83.79178837459712 | total_episodes = 2337 | fit/surrogate_loss = -4.864581108093262 | fit/entropy_loss = 0.6474515795707703 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 272424 | episode_rewards = 83.79178837459712 | total_episodes = 2337 | fit/surrogate_loss = -4.864581108093262 | fit/entropy_loss = 0.6474515795707703 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 273622 | episode_rewards = 0.0 | total_episodes = 2347 | fit/surrogate_loss = -4.864581108093262 | fit/entropy_loss = 0.7062844634056091 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 273622 | episode_rewards = 0.0 | total_episodes = 2347 | fit/surrogate_loss = -4.864581108093262 | fit/entropy_loss = 0.7062844634056091 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 274797 | episode_rewards = 0.0 | total_episodes = 2359 | fit/surrogate_loss = 11.24902057647705 | fit/entropy_loss = 0.647120475769043 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 274797 | episode_rewards = 0.0 | total_episodes = 2359 | fit/surrogate_loss = 11.24902057647705 | fit/entropy_loss = 0.647120475769043 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 275873 | episode_rewards = 0.0 | total_episodes = 2369 | fit/surrogate_loss = -5.041299819946289 | fit/entropy_loss = 0.7615529894828796 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 275873 | episode_rewards = 0.0 | total_episodes = 2369 | fit/surrogate_loss = -5.041299819946289 | fit/entropy_loss = 0.7615529894828796 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 276772 | episode_rewards = 146.6842228837379 | total_episodes = 2378 | fit/surrogate_loss = -15.248852729797363 | fit/entropy_loss = 0.7045814394950867 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 276772 | episode_rewards = 146.6842228837379 | total_episodes = 2378 | fit/surrogate_loss = -15.248852729797363 | fit/entropy_loss = 0.7045814394950867 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 277749 | episode_rewards = 0.0 | total_episodes = 2387 | fit/surrogate_loss = -3.545271635055542 | fit/entropy_loss = 0.6535852551460266 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 277749 | episode_rewards = 0.0 | total_episodes = 2387 | fit/surrogate_loss = -3.545271635055542 | fit/entropy_loss = 0.6535852551460266 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 278827 | episode_rewards = 0.0 | total_episodes = 2398 | fit/surrogate_loss = -2.167829751968384 | fit/entropy_loss = 0.7755787968635559 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 278827 | episode_rewards = 0.0 | total_episodes = 2398 | fit/surrogate_loss = -2.167829751968384 | fit/entropy_loss = 0.7755787968635559 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 279927 | episode_rewards = 0.0 | total_episodes = 2410 | fit/surrogate_loss = -9.187996864318848 | fit/entropy_loss = 0.7755787968635559 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 279927 | episode_rewards = 0.0 | total_episodes = 2410 | fit/surrogate_loss = -9.187996864318848 | fit/entropy_loss = 0.7755787968635559 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 280953 | episode_rewards = 0.0 | total_episodes = 2419 | fit/surrogate_loss = -9.187996864318848 | fit/entropy_loss = 0.661465585231781 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 280953 | episode_rewards = 0.0 | total_episodes = 2419 | fit/surrogate_loss = -9.187996864318848 | fit/entropy_loss = 0.661465585231781 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 281840 | episode_rewards = 0.0 | total_episodes = 2427 | fit/surrogate_loss = -6.680713653564453 | fit/entropy_loss = 0.6728013157844543 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 281840 | episode_rewards = 0.0 | total_episodes = 2427 | fit/surrogate_loss = -6.680713653564453 | fit/entropy_loss = 0.6728013157844543 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 282929 | episode_rewards = 0.0 | total_episodes = 2439 | fit/surrogate_loss = -1.9727977514266968 | fit/entropy_loss = 0.7796775102615356 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 282929 | episode_rewards = 0.0 | total_episodes = 2439 | fit/surrogate_loss = -1.9727977514266968 | fit/entropy_loss = 0.7796775102615356 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 284017 | episode_rewards = 18.0 | total_episodes = 2449 | fit/surrogate_loss = -2.325516700744629 | fit/entropy_loss = 0.6751704812049866 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 284017 | episode_rewards = 18.0 | total_episodes = 2449 | fit/surrogate_loss = -2.325516700744629 | fit/entropy_loss = 0.6751704812049866 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 284956 | episode_rewards = 28.386271574499784 | total_episodes = 2459 | fit/surrogate_loss = 0.7066431045532227 | fit/entropy_loss = 0.6751704812049866 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 284956 | episode_rewards = 28.386271574499784 | total_episodes = 2459 | fit/surrogate_loss = 0.7066431045532227 | fit/entropy_loss = 0.6751704812049866 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 285965 | episode_rewards = 0.0 | total_episodes = 2468 | fit/surrogate_loss = 0.7066431045532227 | fit/entropy_loss = 0.7601300477981567 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 285965 | episode_rewards = 0.0 | total_episodes = 2468 | fit/surrogate_loss = 0.7066431045532227 | fit/entropy_loss = 0.7601300477981567 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 287106 | episode_rewards = 114.9463746682232 | total_episodes = 2478 | fit/surrogate_loss = -4.714165210723877 | fit/entropy_loss = 0.7092376947402954 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 287106 | episode_rewards = 114.9463746682232 | total_episodes = 2478 | fit/surrogate_loss = -4.714165210723877 | fit/entropy_loss = 0.7092376947402954 | \n",
      "\u001b[38;21m[INFO] 18:30: [PPOAgent[worker: 0]] | max_global_step = 288251 | episode_rewards = 154.90000204451187 | total_episodes = 2490 | fit/surrogate_loss = 1.1690926551818848 | fit/entropy_loss = 0.7341939210891724 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 288251 | episode_rewards = 154.90000204451187 | total_episodes = 2490 | fit/surrogate_loss = 1.1690926551818848 | fit/entropy_loss = 0.7341939210891724 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 289212 | episode_rewards = 0.0 | total_episodes = 2499 | fit/surrogate_loss = -4.821784496307373 | fit/entropy_loss = 0.7002788782119751 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 289212 | episode_rewards = 0.0 | total_episodes = 2499 | fit/surrogate_loss = -4.821784496307373 | fit/entropy_loss = 0.7002788782119751 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 290130 | episode_rewards = 103.73160339356082 | total_episodes = 2507 | fit/surrogate_loss = 11.638400077819824 | fit/entropy_loss = 0.662968635559082 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 290130 | episode_rewards = 103.73160339356082 | total_episodes = 2507 | fit/surrogate_loss = 11.638400077819824 | fit/entropy_loss = 0.662968635559082 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 291240 | episode_rewards = 609.8592383739019 | total_episodes = 2519 | fit/surrogate_loss = 11.311269760131836 | fit/entropy_loss = 0.662968635559082 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 291240 | episode_rewards = 609.8592383739019 | total_episodes = 2519 | fit/surrogate_loss = 11.311269760131836 | fit/entropy_loss = 0.662968635559082 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 292424 | episode_rewards = 0.0 | total_episodes = 2529 | fit/surrogate_loss = 11.311269760131836 | fit/entropy_loss = 0.5605802536010742 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 292424 | episode_rewards = 0.0 | total_episodes = 2529 | fit/surrogate_loss = 11.311269760131836 | fit/entropy_loss = 0.5605802536010742 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 293292 | episode_rewards = 0.0 | total_episodes = 2537 | fit/surrogate_loss = 2.12601375579834 | fit/entropy_loss = 0.654970645904541 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 293292 | episode_rewards = 0.0 | total_episodes = 2537 | fit/surrogate_loss = 2.12601375579834 | fit/entropy_loss = 0.654970645904541 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 294262 | episode_rewards = 53.33800314435175 | total_episodes = 2546 | fit/surrogate_loss = -6.5896830558776855 | fit/entropy_loss = 0.586453378200531 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 294262 | episode_rewards = 53.33800314435175 | total_episodes = 2546 | fit/surrogate_loss = -6.5896830558776855 | fit/entropy_loss = 0.586453378200531 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 295353 | episode_rewards = 0.0 | total_episodes = 2556 | fit/surrogate_loss = -2.6655755043029785 | fit/entropy_loss = 0.6088744401931763 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 295353 | episode_rewards = 0.0 | total_episodes = 2556 | fit/surrogate_loss = -2.6655755043029785 | fit/entropy_loss = 0.6088744401931763 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 296435 | episode_rewards = 0.0 | total_episodes = 2566 | fit/surrogate_loss = 9.803048133850098 | fit/entropy_loss = 0.5780272483825684 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 296435 | episode_rewards = 0.0 | total_episodes = 2566 | fit/surrogate_loss = 9.803048133850098 | fit/entropy_loss = 0.5780272483825684 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 297463 | episode_rewards = 245.49622393631452 | total_episodes = 2575 | fit/surrogate_loss = 9.803048133850098 | fit/entropy_loss = 0.5780272483825684 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 297463 | episode_rewards = 245.49622393631452 | total_episodes = 2575 | fit/surrogate_loss = 9.803048133850098 | fit/entropy_loss = 0.5780272483825684 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 298359 | episode_rewards = 0.0 | total_episodes = 2583 | fit/surrogate_loss = 6.011163711547852 | fit/entropy_loss = 0.7014363408088684 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 298359 | episode_rewards = 0.0 | total_episodes = 2583 | fit/surrogate_loss = 6.011163711547852 | fit/entropy_loss = 0.7014363408088684 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 299480 | episode_rewards = 51.89097519114296 | total_episodes = 2595 | fit/surrogate_loss = 6.195263862609863 | fit/entropy_loss = 0.6881054639816284 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 299480 | episode_rewards = 51.89097519114296 | total_episodes = 2595 | fit/surrogate_loss = 6.195263862609863 | fit/entropy_loss = 0.6881054639816284 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 300578 | episode_rewards = 0.0 | total_episodes = 2605 | fit/surrogate_loss = 0.36570167541503906 | fit/entropy_loss = 0.7184164524078369 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 300578 | episode_rewards = 0.0 | total_episodes = 2605 | fit/surrogate_loss = 0.36570167541503906 | fit/entropy_loss = 0.7184164524078369 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 301495 | episode_rewards = 16.0 | total_episodes = 2616 | fit/surrogate_loss = -8.457176208496094 | fit/entropy_loss = 0.5549788475036621 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 301495 | episode_rewards = 16.0 | total_episodes = 2616 | fit/surrogate_loss = -8.457176208496094 | fit/entropy_loss = 0.5549788475036621 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 302172 | episode_rewards = 0.0 | total_episodes = 2622 | fit/surrogate_loss = -8.457176208496094 | fit/entropy_loss = 0.5549788475036621 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 302172 | episode_rewards = 0.0 | total_episodes = 2622 | fit/surrogate_loss = -8.457176208496094 | fit/entropy_loss = 0.5549788475036621 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 303107 | episode_rewards = 0.0 | total_episodes = 2631 | fit/surrogate_loss = 6.755417346954346 | fit/entropy_loss = 0.5536685585975647 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 303107 | episode_rewards = 0.0 | total_episodes = 2631 | fit/surrogate_loss = 6.755417346954346 | fit/entropy_loss = 0.5536685585975647 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 304165 | episode_rewards = 71.37154549099692 | total_episodes = 2641 | fit/surrogate_loss = 7.060032367706299 | fit/entropy_loss = 0.6631330847740173 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 304165 | episode_rewards = 71.37154549099692 | total_episodes = 2641 | fit/surrogate_loss = 7.060032367706299 | fit/entropy_loss = 0.6631330847740173 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 305196 | episode_rewards = 15.0 | total_episodes = 2651 | fit/surrogate_loss = 11.085342407226562 | fit/entropy_loss = 0.7037988901138306 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 305196 | episode_rewards = 15.0 | total_episodes = 2651 | fit/surrogate_loss = 11.085342407226562 | fit/entropy_loss = 0.7037988901138306 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 306235 | episode_rewards = 148.34163789695634 | total_episodes = 2661 | fit/surrogate_loss = 11.085342407226562 | fit/entropy_loss = 0.7037988901138306 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 306235 | episode_rewards = 148.34163789695634 | total_episodes = 2661 | fit/surrogate_loss = 11.085342407226562 | fit/entropy_loss = 0.7037988901138306 | \n",
      "\u001b[38;21m[INFO] 18:31: [PPOAgent[worker: 0]] | max_global_step = 307101 | episode_rewards = 0.0 | total_episodes = 2669 | fit/surrogate_loss = -5.710657596588135 | fit/entropy_loss = 0.7107346653938293 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 307101 | episode_rewards = 0.0 | total_episodes = 2669 | fit/surrogate_loss = -5.710657596588135 | fit/entropy_loss = 0.7107346653938293 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 308225 | episode_rewards = 37.17633561416052 | total_episodes = 2680 | fit/surrogate_loss = -7.183588027954102 | fit/entropy_loss = 0.6991645097732544 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 308225 | episode_rewards = 37.17633561416052 | total_episodes = 2680 | fit/surrogate_loss = -7.183588027954102 | fit/entropy_loss = 0.6991645097732544 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 309332 | episode_rewards = 88.7307861942542 | total_episodes = 2690 | fit/surrogate_loss = 1.7141735553741455 | fit/entropy_loss = 0.671386182308197 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 309332 | episode_rewards = 88.7307861942542 | total_episodes = 2690 | fit/surrogate_loss = 1.7141735553741455 | fit/entropy_loss = 0.671386182308197 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 310315 | episode_rewards = 110.97897455427051 | total_episodes = 2699 | fit/surrogate_loss = 2.741137981414795 | fit/entropy_loss = 0.6828113198280334 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 310315 | episode_rewards = 110.97897455427051 | total_episodes = 2699 | fit/surrogate_loss = 2.741137981414795 | fit/entropy_loss = 0.6828113198280334 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 311188 | episode_rewards = 125.90266114519392 | total_episodes = 2707 | fit/surrogate_loss = 2.741137981414795 | fit/entropy_loss = 0.6828113198280334 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 311188 | episode_rewards = 125.90266114519392 | total_episodes = 2707 | fit/surrogate_loss = 2.741137981414795 | fit/entropy_loss = 0.6828113198280334 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 312347 | episode_rewards = 280.8431055761164 | total_episodes = 2717 | fit/surrogate_loss = 6.733182907104492 | fit/entropy_loss = 0.7109528183937073 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 312347 | episode_rewards = 280.8431055761164 | total_episodes = 2717 | fit/surrogate_loss = 6.733182907104492 | fit/entropy_loss = 0.7109528183937073 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 313483 | episode_rewards = 439.2167760967446 | total_episodes = 2727 | fit/surrogate_loss = -6.7573137283325195 | fit/entropy_loss = 0.6059386134147644 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 313483 | episode_rewards = 439.2167760967446 | total_episodes = 2727 | fit/surrogate_loss = -6.7573137283325195 | fit/entropy_loss = 0.6059386134147644 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 314469 | episode_rewards = 219.9326403141286 | total_episodes = 2736 | fit/surrogate_loss = 2.4972240924835205 | fit/entropy_loss = 0.690179705619812 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 314469 | episode_rewards = 219.9326403141286 | total_episodes = 2736 | fit/surrogate_loss = 2.4972240924835205 | fit/entropy_loss = 0.690179705619812 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 315279 | episode_rewards = 0.0 | total_episodes = 2743 | fit/surrogate_loss = 4.573067665100098 | fit/entropy_loss = 0.717827320098877 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 315279 | episode_rewards = 0.0 | total_episodes = 2743 | fit/surrogate_loss = 4.573067665100098 | fit/entropy_loss = 0.717827320098877 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 316311 | episode_rewards = 210.75342881144064 | total_episodes = 2752 | fit/surrogate_loss = 18.38256072998047 | fit/entropy_loss = 0.5874032378196716 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 316311 | episode_rewards = 210.75342881144064 | total_episodes = 2752 | fit/surrogate_loss = 18.38256072998047 | fit/entropy_loss = 0.5874032378196716 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 317432 | episode_rewards = 386.7407513754355 | total_episodes = 2762 | fit/surrogate_loss = 18.38256072998047 | fit/entropy_loss = 0.5874032378196716 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 317432 | episode_rewards = 386.7407513754355 | total_episodes = 2762 | fit/surrogate_loss = 18.38256072998047 | fit/entropy_loss = 0.5874032378196716 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 318543 | episode_rewards = 0.0 | total_episodes = 2772 | fit/surrogate_loss = 12.315535545349121 | fit/entropy_loss = 0.7151165008544922 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 318543 | episode_rewards = 0.0 | total_episodes = 2772 | fit/surrogate_loss = 12.315535545349121 | fit/entropy_loss = 0.7151165008544922 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 319266 | episode_rewards = 0.0 | total_episodes = 2778 | fit/surrogate_loss = -3.7389328479766846 | fit/entropy_loss = 0.6525595188140869 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 319266 | episode_rewards = 0.0 | total_episodes = 2778 | fit/surrogate_loss = -3.7389328479766846 | fit/entropy_loss = 0.6525595188140869 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 320331 | episode_rewards = 0.0 | total_episodes = 2788 | fit/surrogate_loss = -4.558720111846924 | fit/entropy_loss = 0.6966354250907898 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 320331 | episode_rewards = 0.0 | total_episodes = 2788 | fit/surrogate_loss = -4.558720111846924 | fit/entropy_loss = 0.6966354250907898 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 321449 | episode_rewards = 0.0 | total_episodes = 2798 | fit/surrogate_loss = 2.6404619216918945 | fit/entropy_loss = 0.742167055606842 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 321449 | episode_rewards = 0.0 | total_episodes = 2798 | fit/surrogate_loss = 2.6404619216918945 | fit/entropy_loss = 0.742167055606842 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 322443 | episode_rewards = 17.0 | total_episodes = 2808 | fit/surrogate_loss = -7.9614667892456055 | fit/entropy_loss = 0.742167055606842 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 322443 | episode_rewards = 17.0 | total_episodes = 2808 | fit/surrogate_loss = -7.9614667892456055 | fit/entropy_loss = 0.742167055606842 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 323318 | episode_rewards = 0.0 | total_episodes = 2816 | fit/surrogate_loss = -7.9614667892456055 | fit/entropy_loss = 0.6447027921676636 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 323318 | episode_rewards = 0.0 | total_episodes = 2816 | fit/surrogate_loss = -7.9614667892456055 | fit/entropy_loss = 0.6447027921676636 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 324429 | episode_rewards = -4.0 | total_episodes = 2826 | fit/surrogate_loss = -4.305301189422607 | fit/entropy_loss = 0.783600389957428 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 324429 | episode_rewards = -4.0 | total_episodes = 2826 | fit/surrogate_loss = -4.305301189422607 | fit/entropy_loss = 0.783600389957428 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 325561 | episode_rewards = 17.0 | total_episodes = 2838 | fit/surrogate_loss = -8.62205696105957 | fit/entropy_loss = 0.7201349139213562 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 325561 | episode_rewards = 17.0 | total_episodes = 2838 | fit/surrogate_loss = -8.62205696105957 | fit/entropy_loss = 0.7201349139213562 | \n",
      "\u001b[38;21m[INFO] 18:32: [PPOAgent[worker: 0]] | max_global_step = 326670 | episode_rewards = 241.0699687980082 | total_episodes = 2850 | fit/surrogate_loss = -11.957064628601074 | fit/entropy_loss = 0.7076414227485657 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 326670 | episode_rewards = 241.0699687980082 | total_episodes = 2850 | fit/surrogate_loss = -11.957064628601074 | fit/entropy_loss = 0.7076414227485657 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 327482 | episode_rewards = 5.0 | total_episodes = 2858 | fit/surrogate_loss = 1.8163654804229736 | fit/entropy_loss = 0.7076414227485657 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 327482 | episode_rewards = 5.0 | total_episodes = 2858 | fit/surrogate_loss = 1.8163654804229736 | fit/entropy_loss = 0.7076414227485657 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 328650 | episode_rewards = 0.0 | total_episodes = 2867 | fit/surrogate_loss = 1.8163654804229736 | fit/entropy_loss = 0.6417974829673767 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 328650 | episode_rewards = 0.0 | total_episodes = 2867 | fit/surrogate_loss = 1.8163654804229736 | fit/entropy_loss = 0.6417974829673767 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 329725 | episode_rewards = 0.0 | total_episodes = 2877 | fit/surrogate_loss = 3.8936996459960938 | fit/entropy_loss = 0.649297297000885 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 329725 | episode_rewards = 0.0 | total_episodes = 2877 | fit/surrogate_loss = 3.8936996459960938 | fit/entropy_loss = 0.649297297000885 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 330881 | episode_rewards = 32.84386067639615 | total_episodes = 2887 | fit/surrogate_loss = 7.161426544189453 | fit/entropy_loss = 0.6451199650764465 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 330881 | episode_rewards = 32.84386067639615 | total_episodes = 2887 | fit/surrogate_loss = 7.161426544189453 | fit/entropy_loss = 0.6451199650764465 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 331735 | episode_rewards = 0.0 | total_episodes = 2895 | fit/surrogate_loss = 21.8662109375 | fit/entropy_loss = 0.607747495174408 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 331735 | episode_rewards = 0.0 | total_episodes = 2895 | fit/surrogate_loss = 21.8662109375 | fit/entropy_loss = 0.607747495174408 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 332818 | episode_rewards = 60.38849344417015 | total_episodes = 2905 | fit/surrogate_loss = -6.315805435180664 | fit/entropy_loss = 0.7021433115005493 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 332818 | episode_rewards = 60.38849344417015 | total_episodes = 2905 | fit/surrogate_loss = -6.315805435180664 | fit/entropy_loss = 0.7021433115005493 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 333854 | episode_rewards = 292.2263386249825 | total_episodes = 2914 | fit/surrogate_loss = 4.096222877502441 | fit/entropy_loss = 0.6248108148574829 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 333854 | episode_rewards = 292.2263386249825 | total_episodes = 2914 | fit/surrogate_loss = 4.096222877502441 | fit/entropy_loss = 0.6248108148574829 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 334898 | episode_rewards = 370.54554970574156 | total_episodes = 2925 | fit/surrogate_loss = 13.857733726501465 | fit/entropy_loss = 0.6248108148574829 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 334898 | episode_rewards = 370.54554970574156 | total_episodes = 2925 | fit/surrogate_loss = 13.857733726501465 | fit/entropy_loss = 0.6248108148574829 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 335858 | episode_rewards = 50.622216374292066 | total_episodes = 2933 | fit/surrogate_loss = 13.857733726501465 | fit/entropy_loss = 0.7744174599647522 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 335858 | episode_rewards = 50.622216374292066 | total_episodes = 2933 | fit/surrogate_loss = 13.857733726501465 | fit/entropy_loss = 0.7744174599647522 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 336864 | episode_rewards = 0.0 | total_episodes = 2942 | fit/surrogate_loss = -3.5333168506622314 | fit/entropy_loss = 0.7947954535484314 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 336864 | episode_rewards = 0.0 | total_episodes = 2942 | fit/surrogate_loss = -3.5333168506622314 | fit/entropy_loss = 0.7947954535484314 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 337928 | episode_rewards = 0.0 | total_episodes = 2953 | fit/surrogate_loss = -7.006962299346924 | fit/entropy_loss = 0.7873041033744812 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 337928 | episode_rewards = 0.0 | total_episodes = 2953 | fit/surrogate_loss = -7.006962299346924 | fit/entropy_loss = 0.7873041033744812 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 339013 | episode_rewards = 0.0 | total_episodes = 2965 | fit/surrogate_loss = -10.920823097229004 | fit/entropy_loss = 0.7314431667327881 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 339013 | episode_rewards = 0.0 | total_episodes = 2965 | fit/surrogate_loss = -10.920823097229004 | fit/entropy_loss = 0.7314431667327881 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 339962 | episode_rewards = 0.0 | total_episodes = 2974 | fit/surrogate_loss = -10.920823097229004 | fit/entropy_loss = 0.7314431667327881 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 339962 | episode_rewards = 0.0 | total_episodes = 2974 | fit/surrogate_loss = -10.920823097229004 | fit/entropy_loss = 0.7314431667327881 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 340990 | episode_rewards = 0.0 | total_episodes = 2983 | fit/surrogate_loss = -1.251440405845642 | fit/entropy_loss = 0.7003953456878662 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 340990 | episode_rewards = 0.0 | total_episodes = 2983 | fit/surrogate_loss = -1.251440405845642 | fit/entropy_loss = 0.7003953456878662 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 342068 | episode_rewards = 0.0 | total_episodes = 2993 | fit/surrogate_loss = 16.389854431152344 | fit/entropy_loss = 0.7907335162162781 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 342068 | episode_rewards = 0.0 | total_episodes = 2993 | fit/surrogate_loss = 16.389854431152344 | fit/entropy_loss = 0.7907335162162781 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 343123 | episode_rewards = 0.0 | total_episodes = 3003 | fit/surrogate_loss = -1.6579772233963013 | fit/entropy_loss = 0.771202027797699 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 343123 | episode_rewards = 0.0 | total_episodes = 3003 | fit/surrogate_loss = -1.6579772233963013 | fit/entropy_loss = 0.771202027797699 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 344023 | episode_rewards = 413.08974777085325 | total_episodes = 3011 | fit/surrogate_loss = -0.3645738661289215 | fit/entropy_loss = 0.7953816652297974 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 344023 | episode_rewards = 413.08974777085325 | total_episodes = 3011 | fit/surrogate_loss = -0.3645738661289215 | fit/entropy_loss = 0.7953816652297974 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 344974 | episode_rewards = 163.34554283928517 | total_episodes = 3021 | fit/surrogate_loss = 8.914563179016113 | fit/entropy_loss = 0.7953816652297974 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 344974 | episode_rewards = 163.34554283928517 | total_episodes = 3021 | fit/surrogate_loss = 8.914563179016113 | fit/entropy_loss = 0.7953816652297974 | \n",
      "\u001b[38;21m[INFO] 18:33: [PPOAgent[worker: 0]] | max_global_step = 346105 | episode_rewards = -10.0 | total_episodes = 3031 | fit/surrogate_loss = 8.914563179016113 | fit/entropy_loss = 0.8137231469154358 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 346105 | episode_rewards = -10.0 | total_episodes = 3031 | fit/surrogate_loss = 8.914563179016113 | fit/entropy_loss = 0.8137231469154358 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 347263 | episode_rewards = 623.0656443521328 | total_episodes = 3041 | fit/surrogate_loss = -10.402698516845703 | fit/entropy_loss = 0.8875650763511658 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 347263 | episode_rewards = 623.0656443521328 | total_episodes = 3041 | fit/surrogate_loss = -10.402698516845703 | fit/entropy_loss = 0.8875650763511658 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 348156 | episode_rewards = 301.4810077085794 | total_episodes = 3049 | fit/surrogate_loss = 10.756000518798828 | fit/entropy_loss = 0.8392208218574524 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 348156 | episode_rewards = 301.4810077085794 | total_episodes = 3049 | fit/surrogate_loss = 10.756000518798828 | fit/entropy_loss = 0.8392208218574524 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 349139 | episode_rewards = 0.0 | total_episodes = 3058 | fit/surrogate_loss = 11.080799102783203 | fit/entropy_loss = 0.7688179612159729 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 349139 | episode_rewards = 0.0 | total_episodes = 3058 | fit/surrogate_loss = 11.080799102783203 | fit/entropy_loss = 0.7688179612159729 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 350285 | episode_rewards = 0.0 | total_episodes = 3069 | fit/surrogate_loss = 1.7985327243804932 | fit/entropy_loss = 0.8113245964050293 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 350285 | episode_rewards = 0.0 | total_episodes = 3069 | fit/surrogate_loss = 1.7985327243804932 | fit/entropy_loss = 0.8113245964050293 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 351454 | episode_rewards = 717.3796344999537 | total_episodes = 3079 | fit/surrogate_loss = 15.69039535522461 | fit/entropy_loss = 0.7094306349754333 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 351454 | episode_rewards = 717.3796344999537 | total_episodes = 3079 | fit/surrogate_loss = 15.69039535522461 | fit/entropy_loss = 0.7094306349754333 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 352435 | episode_rewards = 0.0 | total_episodes = 3088 | fit/surrogate_loss = 15.69039535522461 | fit/entropy_loss = 0.7094306349754333 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 352435 | episode_rewards = 0.0 | total_episodes = 3088 | fit/surrogate_loss = 15.69039535522461 | fit/entropy_loss = 0.7094306349754333 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 353465 | episode_rewards = 0.0 | total_episodes = 3097 | fit/surrogate_loss = 14.917067527770996 | fit/entropy_loss = 0.815872311592102 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 353465 | episode_rewards = 0.0 | total_episodes = 3097 | fit/surrogate_loss = 14.917067527770996 | fit/entropy_loss = 0.815872311592102 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 354528 | episode_rewards = 0.0 | total_episodes = 3107 | fit/surrogate_loss = 6.613362789154053 | fit/entropy_loss = 0.8499997854232788 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 354528 | episode_rewards = 0.0 | total_episodes = 3107 | fit/surrogate_loss = 6.613362789154053 | fit/entropy_loss = 0.8499997854232788 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 355632 | episode_rewards = -2.0 | total_episodes = 3117 | fit/surrogate_loss = -6.261346817016602 | fit/entropy_loss = 0.8597565293312073 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 355632 | episode_rewards = -2.0 | total_episodes = 3117 | fit/surrogate_loss = -6.261346817016602 | fit/entropy_loss = 0.8597565293312073 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 356494 | episode_rewards = 0.0 | total_episodes = 3125 | fit/surrogate_loss = -9.891491889953613 | fit/entropy_loss = 0.7340006232261658 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 356494 | episode_rewards = 0.0 | total_episodes = 3125 | fit/surrogate_loss = -9.891491889953613 | fit/entropy_loss = 0.7340006232261658 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 357412 | episode_rewards = 469.52880966143147 | total_episodes = 3133 | fit/surrogate_loss = -9.891491889953613 | fit/entropy_loss = 0.7340006232261658 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 357412 | episode_rewards = 469.52880966143147 | total_episodes = 3133 | fit/surrogate_loss = -9.891491889953613 | fit/entropy_loss = 0.7340006232261658 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 358519 | episode_rewards = 0.0 | total_episodes = 3145 | fit/surrogate_loss = 2.952632188796997 | fit/entropy_loss = 0.7887834906578064 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 358519 | episode_rewards = 0.0 | total_episodes = 3145 | fit/surrogate_loss = 2.952632188796997 | fit/entropy_loss = 0.7887834906578064 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 359592 | episode_rewards = 0.0 | total_episodes = 3155 | fit/surrogate_loss = -14.404336929321289 | fit/entropy_loss = 0.8432538509368896 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 359592 | episode_rewards = 0.0 | total_episodes = 3155 | fit/surrogate_loss = -14.404336929321289 | fit/entropy_loss = 0.8432538509368896 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 360569 | episode_rewards = 0.0 | total_episodes = 3165 | fit/surrogate_loss = -6.443353176116943 | fit/entropy_loss = 0.7982860803604126 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 360569 | episode_rewards = 0.0 | total_episodes = 3165 | fit/surrogate_loss = -6.443353176116943 | fit/entropy_loss = 0.7982860803604126 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 361447 | episode_rewards = 79.47176653717652 | total_episodes = 3173 | fit/surrogate_loss = -10.937483787536621 | fit/entropy_loss = 0.8991641998291016 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 361447 | episode_rewards = 79.47176653717652 | total_episodes = 3173 | fit/surrogate_loss = -10.937483787536621 | fit/entropy_loss = 0.8991641998291016 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 362539 | episode_rewards = 185.5314832117562 | total_episodes = 3183 | fit/surrogate_loss = -11.130887985229492 | fit/entropy_loss = 0.8003912568092346 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 362539 | episode_rewards = 185.5314832117562 | total_episodes = 3183 | fit/surrogate_loss = -11.130887985229492 | fit/entropy_loss = 0.8003912568092346 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 363741 | episode_rewards = 0.0 | total_episodes = 3194 | fit/surrogate_loss = -11.130887985229492 | fit/entropy_loss = 0.8003912568092346 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 363741 | episode_rewards = 0.0 | total_episodes = 3194 | fit/surrogate_loss = -11.130887985229492 | fit/entropy_loss = 0.8003912568092346 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 364771 | episode_rewards = 618.1249458803964 | total_episodes = 3203 | fit/surrogate_loss = 0.4867091178894043 | fit/entropy_loss = 0.8650573492050171 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 364771 | episode_rewards = 618.1249458803964 | total_episodes = 3203 | fit/surrogate_loss = 0.4867091178894043 | fit/entropy_loss = 0.8650573492050171 | \n",
      "\u001b[38;21m[INFO] 18:34: [PPOAgent[worker: 0]] | max_global_step = 365644 | episode_rewards = 0.0 | total_episodes = 3211 | fit/surrogate_loss = 11.193117141723633 | fit/entropy_loss = 0.8364806771278381 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 365644 | episode_rewards = 0.0 | total_episodes = 3211 | fit/surrogate_loss = 11.193117141723633 | fit/entropy_loss = 0.8364806771278381 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 366705 | episode_rewards = 0.0 | total_episodes = 3222 | fit/surrogate_loss = -0.5744564533233643 | fit/entropy_loss = 0.8350659608840942 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 366705 | episode_rewards = 0.0 | total_episodes = 3222 | fit/surrogate_loss = -0.5744564533233643 | fit/entropy_loss = 0.8350659608840942 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 367810 | episode_rewards = 632.2940730780455 | total_episodes = 3232 | fit/surrogate_loss = -6.811185836791992 | fit/entropy_loss = 0.8497910499572754 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 367810 | episode_rewards = 632.2940730780455 | total_episodes = 3232 | fit/surrogate_loss = -6.811185836791992 | fit/entropy_loss = 0.8497910499572754 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 368867 | episode_rewards = 0.0 | total_episodes = 3242 | fit/surrogate_loss = 3.719151020050049 | fit/entropy_loss = 0.8904508352279663 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 368867 | episode_rewards = 0.0 | total_episodes = 3242 | fit/surrogate_loss = 3.719151020050049 | fit/entropy_loss = 0.8904508352279663 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 369692 | episode_rewards = 94.34494017063163 | total_episodes = 3250 | fit/surrogate_loss = 3.719151020050049 | fit/entropy_loss = 0.8904508352279663 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 369692 | episode_rewards = 94.34494017063163 | total_episodes = 3250 | fit/surrogate_loss = 3.719151020050049 | fit/entropy_loss = 0.8904508352279663 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 370772 | episode_rewards = 103.38688454859056 | total_episodes = 3260 | fit/surrogate_loss = 3.564878225326538 | fit/entropy_loss = 0.8865048289299011 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 370772 | episode_rewards = 103.38688454859056 | total_episodes = 3260 | fit/surrogate_loss = 3.564878225326538 | fit/entropy_loss = 0.8865048289299011 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 371873 | episode_rewards = -12.0 | total_episodes = 3270 | fit/surrogate_loss = -9.410504341125488 | fit/entropy_loss = 0.7910668253898621 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 371873 | episode_rewards = -12.0 | total_episodes = 3270 | fit/surrogate_loss = -9.410504341125488 | fit/entropy_loss = 0.7910668253898621 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 372970 | episode_rewards = 0.0 | total_episodes = 3280 | fit/surrogate_loss = -9.403847694396973 | fit/entropy_loss = 0.7464576959609985 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 372970 | episode_rewards = 0.0 | total_episodes = 3280 | fit/surrogate_loss = -9.403847694396973 | fit/entropy_loss = 0.7464576959609985 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 373732 | episode_rewards = 457.3289296552667 | total_episodes = 3289 | fit/surrogate_loss = -9.403847694396973 | fit/entropy_loss = 0.7464576959609985 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 373732 | episode_rewards = 457.3289296552667 | total_episodes = 3289 | fit/surrogate_loss = -9.403847694396973 | fit/entropy_loss = 0.7464576959609985 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 374813 | episode_rewards = 0.0 | total_episodes = 3300 | fit/surrogate_loss = 9.735477447509766 | fit/entropy_loss = 0.838747501373291 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 374813 | episode_rewards = 0.0 | total_episodes = 3300 | fit/surrogate_loss = 9.735477447509766 | fit/entropy_loss = 0.838747501373291 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 375820 | episode_rewards = 78.97675202910484 | total_episodes = 3309 | fit/surrogate_loss = -2.555683135986328 | fit/entropy_loss = 0.8246271014213562 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 375820 | episode_rewards = 78.97675202910484 | total_episodes = 3309 | fit/surrogate_loss = -2.555683135986328 | fit/entropy_loss = 0.8246271014213562 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 376845 | episode_rewards = 0.0 | total_episodes = 3319 | fit/surrogate_loss = 8.954416275024414 | fit/entropy_loss = 0.9192997217178345 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 376845 | episode_rewards = 0.0 | total_episodes = 3319 | fit/surrogate_loss = 8.954416275024414 | fit/entropy_loss = 0.9192997217178345 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 377675 | episode_rewards = 0.0 | total_episodes = 3327 | fit/surrogate_loss = -2.8439855575561523 | fit/entropy_loss = 0.8421152830123901 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 377675 | episode_rewards = 0.0 | total_episodes = 3327 | fit/surrogate_loss = -2.8439855575561523 | fit/entropy_loss = 0.8421152830123901 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 378702 | episode_rewards = 0.0 | total_episodes = 3337 | fit/surrogate_loss = -3.6510567665100098 | fit/entropy_loss = 0.8421152830123901 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 378702 | episode_rewards = 0.0 | total_episodes = 3337 | fit/surrogate_loss = -3.6510567665100098 | fit/entropy_loss = 0.8421152830123901 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 379928 | episode_rewards = 0.0 | total_episodes = 3349 | fit/surrogate_loss = -3.6510567665100098 | fit/entropy_loss = 0.8199917674064636 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 379928 | episode_rewards = 0.0 | total_episodes = 3349 | fit/surrogate_loss = -3.6510567665100098 | fit/entropy_loss = 0.8199917674064636 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 381007 | episode_rewards = 16.0 | total_episodes = 3359 | fit/surrogate_loss = -2.2067625522613525 | fit/entropy_loss = 0.9402965307235718 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 381007 | episode_rewards = 16.0 | total_episodes = 3359 | fit/surrogate_loss = -2.2067625522613525 | fit/entropy_loss = 0.9402965307235718 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 381776 | episode_rewards = 0.0 | total_episodes = 3366 | fit/surrogate_loss = -6.343733787536621 | fit/entropy_loss = 0.8594657182693481 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 381776 | episode_rewards = 0.0 | total_episodes = 3366 | fit/surrogate_loss = -6.343733787536621 | fit/entropy_loss = 0.8594657182693481 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 382836 | episode_rewards = -19.199426924764992 | total_episodes = 3376 | fit/surrogate_loss = -5.064687728881836 | fit/entropy_loss = 0.8320293426513672 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 382836 | episode_rewards = -19.199426924764992 | total_episodes = 3376 | fit/surrogate_loss = -5.064687728881836 | fit/entropy_loss = 0.8320293426513672 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 383932 | episode_rewards = -6.0 | total_episodes = 3386 | fit/surrogate_loss = 3.0819780826568604 | fit/entropy_loss = 0.8086267709732056 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 383932 | episode_rewards = -6.0 | total_episodes = 3386 | fit/surrogate_loss = 3.0819780826568604 | fit/entropy_loss = 0.8086267709732056 | \n",
      "\u001b[38;21m[INFO] 18:35: [PPOAgent[worker: 0]] | max_global_step = 384973 | episode_rewards = 0.0 | total_episodes = 3397 | fit/surrogate_loss = -4.053352355957031 | fit/entropy_loss = 0.8086267709732056 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 384973 | episode_rewards = 0.0 | total_episodes = 3397 | fit/surrogate_loss = -4.053352355957031 | fit/entropy_loss = 0.8086267709732056 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 385979 | episode_rewards = -68.0 | total_episodes = 3405 | fit/surrogate_loss = -4.053352355957031 | fit/entropy_loss = 0.7843354344367981 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 385979 | episode_rewards = -68.0 | total_episodes = 3405 | fit/surrogate_loss = -4.053352355957031 | fit/entropy_loss = 0.7843354344367981 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 386988 | episode_rewards = 0.0 | total_episodes = 3415 | fit/surrogate_loss = -3.567572832107544 | fit/entropy_loss = 0.7475117444992065 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 386988 | episode_rewards = 0.0 | total_episodes = 3415 | fit/surrogate_loss = -3.567572832107544 | fit/entropy_loss = 0.7475117444992065 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 388033 | episode_rewards = 0.0 | total_episodes = 3426 | fit/surrogate_loss = -0.8842945098876953 | fit/entropy_loss = 0.8114681839942932 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 388033 | episode_rewards = 0.0 | total_episodes = 3426 | fit/surrogate_loss = -0.8842945098876953 | fit/entropy_loss = 0.8114681839942932 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 388861 | episode_rewards = 120.87122854235525 | total_episodes = 3433 | fit/surrogate_loss = -0.09577185660600662 | fit/entropy_loss = 0.7731322646141052 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 388861 | episode_rewards = 120.87122854235525 | total_episodes = 3433 | fit/surrogate_loss = -0.09577185660600662 | fit/entropy_loss = 0.7731322646141052 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 389765 | episode_rewards = -10.0 | total_episodes = 3441 | fit/surrogate_loss = -0.09577185660600662 | fit/entropy_loss = 0.7731322646141052 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 389765 | episode_rewards = -10.0 | total_episodes = 3441 | fit/surrogate_loss = -0.09577185660600662 | fit/entropy_loss = 0.7731322646141052 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 390780 | episode_rewards = 0.0 | total_episodes = 3450 | fit/surrogate_loss = -3.0206186771392822 | fit/entropy_loss = 0.8008032441139221 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 390780 | episode_rewards = 0.0 | total_episodes = 3450 | fit/surrogate_loss = -3.0206186771392822 | fit/entropy_loss = 0.8008032441139221 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 391873 | episode_rewards = 501.3369110846573 | total_episodes = 3461 | fit/surrogate_loss = 8.625277519226074 | fit/entropy_loss = 0.8323312401771545 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 391873 | episode_rewards = 501.3369110846573 | total_episodes = 3461 | fit/surrogate_loss = 8.625277519226074 | fit/entropy_loss = 0.8323312401771545 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 392947 | episode_rewards = 252.4661534354366 | total_episodes = 3472 | fit/surrogate_loss = 7.567429065704346 | fit/entropy_loss = 0.8511138558387756 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 392947 | episode_rewards = 252.4661534354366 | total_episodes = 3472 | fit/surrogate_loss = 7.567429065704346 | fit/entropy_loss = 0.8511138558387756 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 393847 | episode_rewards = 776.6709416919033 | total_episodes = 3480 | fit/surrogate_loss = 0.535155177116394 | fit/entropy_loss = 0.8414456248283386 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 393847 | episode_rewards = 776.6709416919033 | total_episodes = 3480 | fit/surrogate_loss = 0.535155177116394 | fit/entropy_loss = 0.8414456248283386 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 394845 | episode_rewards = 173.86845362425245 | total_episodes = 3490 | fit/surrogate_loss = 0.535155177116394 | fit/entropy_loss = 0.8414456248283386 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 394845 | episode_rewards = 173.86845362425245 | total_episodes = 3490 | fit/surrogate_loss = 0.535155177116394 | fit/entropy_loss = 0.8414456248283386 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 395903 | episode_rewards = 0.0 | total_episodes = 3499 | fit/surrogate_loss = 16.72565269470215 | fit/entropy_loss = 0.8031198382377625 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 395903 | episode_rewards = 0.0 | total_episodes = 3499 | fit/surrogate_loss = 16.72565269470215 | fit/entropy_loss = 0.8031198382377625 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 396997 | episode_rewards = 0.0 | total_episodes = 3510 | fit/surrogate_loss = -7.022933006286621 | fit/entropy_loss = 0.8787598609924316 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 396997 | episode_rewards = 0.0 | total_episodes = 3510 | fit/surrogate_loss = -7.022933006286621 | fit/entropy_loss = 0.8787598609924316 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 398006 | episode_rewards = 116.54996255438226 | total_episodes = 3519 | fit/surrogate_loss = -3.3370602130889893 | fit/entropy_loss = 0.8382751941680908 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 398006 | episode_rewards = 116.54996255438226 | total_episodes = 3519 | fit/surrogate_loss = -3.3370602130889893 | fit/entropy_loss = 0.8382751941680908 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 398930 | episode_rewards = 79.0377816399052 | total_episodes = 3527 | fit/surrogate_loss = 17.93332290649414 | fit/entropy_loss = 0.8981310129165649 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 398930 | episode_rewards = 79.0377816399052 | total_episodes = 3527 | fit/surrogate_loss = 17.93332290649414 | fit/entropy_loss = 0.8981310129165649 | \n",
      "\u001b[38;21m[INFO] 18:36: [PPOAgent[worker: 0]] | max_global_step = 399995 | episode_rewards = 0.0 | total_episodes = 3538 | fit/surrogate_loss = 10.712874412536621 | fit/entropy_loss = 0.8981310129165649 |  \u001b[0m\n",
      "INFO:rlberry_logger:[PPOAgent[worker: 0]] | max_global_step = 399995 | episode_rewards = 0.0 | total_episodes = 3538 | fit/surrogate_loss = 10.712874412536621 | fit/entropy_loss = 0.8981310129165649 | \n",
      "\u001b[38;21m[INFO] 18:36: ... trained! \u001b[0m\n",
      "INFO:rlberry_logger:... trained!\n",
      "/usr/local/lib/python3.8/dist-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "manager = AgentManager(\n",
    "        PPOAgent,\n",
    "        (env_ctor, env_kwargs),\n",
    "        agent_name=\"PPOAgent\",\n",
    "        init_kwargs=dict(\n",
    "            policy_net_fn=model_factory_from_env,\n",
    "            policy_net_kwargs=policy_configs,\n",
    "            value_net_fn=model_factory_from_env,\n",
    "            value_net_kwargs=value_configs,\n",
    "            learning_rate=1e-5,\n",
    "            n_steps=5 * 250,\n",
    "            batch_size=250,\n",
    "            eps_clip=0.2,\n",
    "        ),\n",
    "        fit_budget=4e5,\n",
    "        eval_kwargs=dict(eval_horizon=365),\n",
    "        n_fit=1,\n",
    "        output_dir=\"ppo1_results\", # results/trained agents are kept in this directory\n",
    "        seed = 1\n",
    "    )\n",
    "manager.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b148f-6769-4b7c-8961-5208b3edff2f",
   "metadata": {
    "id": "857b148f-6769-4b7c-8961-5208b3edff2f"
   },
   "source": [
    "### Training reward curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6770442-c396-4f05-acaf-072a1fda2c90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "a6770442-c396-4f05-acaf-072a1fda2c90",
    "outputId": "97d03995-0561-4344-bed2-72efb12c2cb8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(-150,250)\n",
    "data = plot_writer_data(manager, tag=\"episode_rewards\", smooth_weight=0.95, ax = ax) # smoothing tensorboard-style\n",
    "fig.savefig(\"ppo_rewards.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1bc02-d647-48e8-9c17-e9fbf79f297b",
   "metadata": {
    "id": "d6d1bc02-d647-48e8-9c17-e9fbf79f297b"
   },
   "source": [
    "### Evaluation of the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5dd296",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "2a5dd296",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b7a72ca9-3b06-456f-d4ef-a5aed0103c65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m[INFO] 18:36: Evaluating PPOAgent... \u001b[0m\n",
      "INFO:rlberry_logger:Evaluating PPOAgent...\n",
      "[INFO] Evaluation:INFO:rlberry_logger:[INFO] Evaluation:\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      ".INFO:rlberry_logger:.\n",
      "  Evaluation finished \n",
      "INFO:rlberry_logger:  Evaluation finished \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e54a25e3-bcef-4cb1-bb7c-2830d47f58c5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PPOAgent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94.407758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>55.496212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-11.250372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54.927229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>85.600174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>126.018263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>307.420823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e54a25e3-bcef-4cb1-bb7c-2830d47f58c5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e54a25e3-bcef-4cb1-bb7c-2830d47f58c5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e54a25e3-bcef-4cb1-bb7c-2830d47f58c5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         PPOAgent\n",
       "count  128.000000\n",
       "mean    94.407758\n",
       "std     55.496212\n",
       "min    -11.250372\n",
       "25%     54.927229\n",
       "50%     85.600174\n",
       "75%    126.018263\n",
       "max    307.420823"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = evaluate_agents([manager], n_simulations=128, plot=False)\n",
    "evaluation.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fO31WliiKShz",
   "metadata": {
    "id": "fO31WliiKShz"
   },
   "source": [
    "Remark : only one seed was used, more seeds should be used to have a more accurate evaluation. Specify n_fits > 1 for this in the AgentManager above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2362e-2633-4851-a1b8-248fe3df0534",
   "metadata": {
    "id": "c7a2362e-2633-4851-a1b8-248fe3df0534"
   },
   "source": [
    "### Small peek into the agents policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1cb69-2847-4f88-a631-914716a15e4c",
   "metadata": {
    "id": "4cb1cb69-2847-4f88-a631-914716a15e4c"
   },
   "outputs": [],
   "source": [
    "agent = manager.agent_handlers[0] # select the agent from the manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e915f0-72c4-4ba1-bc2f-1fad5ada10ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8e915f0-72c4-4ba1-bc2f-1fad5ada10ac",
    "outputId": "ecfe2741-6632-49da-c81c-e61f220b30d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "env = env_ctor(**env_kwargs)\n",
    "obs = env.reset()\n",
    "\n",
    "actions_txt = [\"doing nothing\", \n",
    "           \"1L of water\", \n",
    "           \"5L of water\", \n",
    "           \"harvesting\",\n",
    "           \"sow some seeds\",\n",
    "           \"scatter fertilizer\",\n",
    "           \"scatter herbicide\",\n",
    "           \"scatter pesticide\",\n",
    "           \"remove weeds by hand\",]\n",
    "episode = pd.DataFrame()\n",
    "\n",
    "for _ in range(100):\n",
    "  is_done = False\n",
    "  while not is_done :\n",
    "      action = agent.policy(obs)    \n",
    "      obs,reward, is_done,_ =  env.step(action)\n",
    "      episode = pd.concat([episode, pd.DataFrame({'action':[actions_txt[action]],\n",
    "                                                  'reward':[reward]})], ignore_index=True)\n",
    "  env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707f129-5d05-480d-9e72-68258fb56584",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "9707f129-5d05-480d-9e72-68258fb56584",
    "outputId": "78d0d022-2476-4625-ddb0-d90035d60977"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "sns.countplot(data = episode, x = \"action\", order = episode['action'].value_counts().index)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "fig.savefig('ppo_actions.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc4a5a-8ef3-4f1b-93e8-09772582dc1c",
   "metadata": {
    "id": "dadc4a5a-8ef3-4f1b-93e8-09772582dc1c"
   },
   "source": [
    "This agent did not learn how to put pesticide and herbicide, it learned that pesticide and herbicide can bring negative reward. On the other hand, it learned that fertilizer was important to have healthy plants.\n",
    "\n",
    "Remark that sowing seeds does nothing after the first time (once the plant is already there) and similarly, harvesting does nothing if the plant do not bear fruits yet. Hence, the agent learned that doing nothing is not a bad policy most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2H4d21SMN5iG",
   "metadata": {
    "id": "2H4d21SMN5iG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "85a459295b2d249e5b14d5bfb5d9f31f2e6a00fba095226dad90be93b01ab22c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
